[{"pk": 2, "model": "blog.blogpost", "fields": {"content": "Last night I gave a talk at the third SyDjango meetup where I presented my django-forms-builder project. Below is the video and slides from the talk which went really well, spurring about a dozen questions at the end. Excuse the background noise in the video, unfortunately we didn't have the venue to ourselves. Thanks to all the fellow Sydney Djangonauts who came along and made it a great night.\r\n\r\nTable of Contents\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nTable of contents and video/slides sync via vimeo-deck.", "published": 0, "publish_date": "2012-09-21T07:09:03", "slug": "sydjango-talk-django-forms-builder", "title": "SyDjango Talk: django-forms-builder"}}, {"pk": 3, "model": "blog.blogpost", "fields": {"content": "The Django Dash for 2012 is now over, and once again it was a ton of fun! The Dash is a 48 hour coding competition for teams of up to three, competing to see who can build the best Django application over a weekend. To top it off, all of the entries are made available as open source.\r\n\r\nThis year I went solo for the first time, which made it quite gruelling as I had to handle everything from back-end code, interface development, and visual design. I decided to follow the same recipe as our 3rd place entry from last year, Drawn By, specifically by creating an environment where people can\r\ncome together and interact in real-time using WebSockets. I named my entry GAMBLOR - it's an online casino where people can move around and chat with each other, while playing casino games with fake money. Check out GAMBLOR as well as the source code if you're interested, and you can see the game plugin system I built to power the initial games implemented: roulette and craps.\r\n\r\nApart from the staples of Django and jQuery, here are some of the components I used that made GAMBLOR possible in such a short amount of time:\r\n\r\n\r\n  gevent-socketio - Provides the Python implementation of socket.io, allowing real-time activity to occur in the browser using WebSockets.\r\n  Redis - A NoSQL database, similar to memcached but with richer data structures.\r\n  CSS3 - The design was implemented almost entirely without graphics, using many CSS3 features such as shadows, background gradients, and even 3D transforms for the gaming chips.\r\n  jquery-transit - A library for advanced JavaScript animations, which powered the roulette wheel rotation.\r\n  jquery-collision - Provides collision detection for browser objects, which allowed for avatar movement to be constrained within the game area and around the game tables.\r\n\r\n\r\nI've put together a table below that lists all of the final entries, with links to their sites and source code. I left out any entries that didn't contain a working site, or didn't seem to function. If I've missed any or you have any other corrections, please let me know.\r\n\r\nIt's interesting to look at some of the recurring themes present within all of the entries. There were multiple entries that fell under each of the following ideas:\r\n\r\n\r\n  Cloud server management - Cloud Fish, Gungnir\r\n  GitHub mashups - Badger, Heroes of Git &amp; Hub, Mosaic, Try Box\r\n  Tutorial/class creation - Django Tutorial, Try Box, Try Try, Tutor Us\r\n  Photo mashups - Gif Feed, Green Room, Lemidora, Miracles Live, The Busitizer\r\n  Portfolio generation - Folio Bag, Mosaic\r\n  Map mashups - Kirchenreich, Quester\r\n\r\n\r\nMy definite favourites are Heroes of Git &amp; Hub, which lets you battle your open source projects against others in a D&amp;D style game, and The Busitizer which I'll just leave for you to check out for yourself.\r\n\r\n\r\n\r\n    Title\r\n    Description\r\n    Team Size\r\n    \r\n    \r\n\r\n\r\n  API Tester\r\n  Automated tests for APIs\r\n  3\r\n  Site\r\n  Source\r\n\r\n\r\n  Badger\r\n  Get badges for open source\r\n  3\r\n  Site\r\n  Source\r\n\r\n\r\n  Black Hole\r\n  Mail web-app\r\n  3\r\n  Site\r\n  Source\r\n\r\n\r\n  Cloud Fish\r\n  Cloud server manager\r\n  3\r\n  Site\r\n  Source\r\n\r\n\r\n  Comminator\r\n  Hacker News clone\r\n  1\r\n  Site\r\n  Source\r\n\r\n\r\n  Crowd Photo\r\n  Crowd sourcing photos\r\n  2\r\n  Site\r\n  Source\r\n\r\n\r\n  Django Gallery\r\n  App store for photos\r\n  1\r\n  Site\r\n  Source\r\n\r\n\r\n  Django Tutorial\r\n  Interactive Django tutorial\r\n  3\r\n  Site\r\n  Source\r\n\r\n\r\n  Folio Bag\r\n  Portfolio generator\r\n  1\r\n  Site\r\n  Source\r\n\r\n\r\n  Folivora\r\n  Dependency manager\r\n  3\r\n  Site\r\n  Source\r\n\r\n\r\n  GAMBLOR\r\n  Real-time multi-user casino\r\n  1\r\n  Site\r\n  Source\r\n\r\n\r\n  Gif Feed\r\n  Collection of Gifs\r\n  1\r\n  Site\r\n  Source\r\n\r\n\r\n  Green Room\r\n  Dressing room opinion app\r\n  3\r\n  Site\r\n  Source\r\n\r\n\r\n  Gungnir\r\n  Cloud server manager\r\n  3\r\n  Site\r\n  Source\r\n\r\n\r\n  Heroes of Git &amp; Hub\r\n  Battles of Github projects\r\n  2\r\n  Site\r\n  Source\r\n\r\n\r\n  Ipse Dixit\r\n  Collection of quotes\r\n  3\r\n  Site\r\n  Source\r\n\r\n\r\n  Kirchenreich\r\n  Maps mashup for churches\r\n  3\r\n  Site\r\n  Source\r\n\r\n\r\n  Lemidora\r\n  Photo sharing\r\n  3\r\n  Site\r\n  Source\r\n\r\n\r\n  Lictor\r\n  Stack trace visualizer\r\n  3\r\n  Site\r\n  Source\r\n\r\n\r\n  Like I'm 5ive\r\n  Dictionary / wiki tool\r\n  3\r\n  Site\r\n  Source\r\n\r\n\r\n  Miracles Live\r\n  Instagram/Flickr mashup of world wonders\r\n  3\r\n  Site\r\n  Source\r\n\r\n\r\n  Mosaic\r\n  Open source project portfolio generator\r\n  2\r\n  Site\r\n  Source\r\n\r\n\r\n  Old Mail\r\n  Group email collboration\r\n  3\r\n  Site\r\n  Source\r\n\r\n\r\n  Pelican Migrator\r\n  Migrate blogs to Pelican\r\n  3\r\n  Site\r\n  Source\r\n\r\n\r\n  Project Starter\r\n  Landing page generator\r\n  2\r\n  Site\r\n  Source\r\n\r\n\r\n  Promisely\r\n  Collection of promises\r\n  3\r\n  Site\r\n  Source\r\n\r\n\r\n  Publican\r\n  Business ledger app\r\n  1\r\n  Site\r\n  Source\r\n\r\n\r\n  Quester\r\n  Create quest games on a world map\r\n  3\r\n  Site\r\n  Source\r\n\r\n\r\n  Red Check\r\n  Spell checker for websites\r\n  3\r\n  Site\r\n  Source\r\n\r\n\r\n  Slug.in\r\n  Custom URL shortener\r\n  3\r\n  Site\r\n  Source\r\n\r\n\r\n  Tamli\r\n  Social bookmarking\r\n  1\r\n  Site\r\n  Source\r\n\r\n\r\n  The Busitizer\r\n  Add Gary Busey to Facebook photos\r\n  3\r\n  Site\r\n  Source\r\n\r\n\r\n  The Hack Box\r\n  Hackathon manager\r\n  3\r\n  Site\r\n  Source\r\n\r\n\r\n  Try Box\r\n  Online IDE for programming tutorials\r\n  3\r\n  Site\r\n  Source\r\n\r\n\r\n  Try Try\r\n  Interactive programming tutorials\r\n  2\r\n  Site\r\n  Source\r\n\r\n\r\n  Tutor Us\r\n  Create &amp; run online classes\r\n  3\r\n  Site\r\n  Source\r\n\r\n\r\n  Wikipedia Analytics\r\n  Create charts from Wikipedia data\r\n  2\r\n  Site\r\n  Source\r\n\r\n", "published": 0, "publish_date": "2012-08-21T07:08:06", "slug": "django-dash-2012-round-up", "title": "Django Dash 2012 Round-Up"}}, {"pk": 4, "model": "blog.blogpost", "fields": {"content": "One of Django's many great features is its powerful template inheritance. A decade ago we would use simple concepts like include files for platforms such as ASP and PHP, allowing us to create reusable template snippets that could be embedded in multiple pages. Later, ASP.NET and Ruby on Rails would improve on this with their master/layout concepts, allowing us to define a base skeleton template for a site, with an area that all other pages would inject their content into. Django takes this approach even further with its template inheritance. It allows templates to extend other (parent) templates, with those parent templates containing named blocks that can be overridden. The blocks in the parent template can contain default content, and when overriding these blocks, the default content can be overridden, left as is, or even prefixed with or appended to, as the child template will have access to the default content in the parent template's block. This is analogous to object oriented programming, where base classes can be subclassed, and have their methods overridden, with access to the super-class's methods to be called at whatever point is deemed appropriate.\n\nOverriding vs Extending\n\nAnother powerful feature of Django's is its template loaders. Each loader implements an approach for finding and loading the contents of a template when it's requested by name. A typical Django project will contain multiple template loaders, and when a template is loaded by name, that name will be passed through each of the loaders sequentially, until one of the loaders finds the template.\n\nThe two most commonly used template loaders are the filesystem loader and the app_directories loader. The filesystem loader will look at the TEMPLATE_DIRS setting, and search each of the directory paths in it for the requested template. The app_directories loader is similar, but will look for a directory called 'templates' in each of the apps listed in the INSTALLED_APPS setting.\n\nTEMPLATE_LOADERS = (\n    \"django.template.loaders.filesystem.Loader\",\n    \"django.template.loaders.app_directories.Loader\",\n)\n\nPROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))\nTEMPLATE_DIRS = (os.path.join(PROJECT_ROOT, \"templates\"),)\n\n\n\nReusable Django apps will often provide a default set of templates where applicable, and the app's view functions will load these templates by name. With both the filesystem and app_directories template loaders configured, the app's version of the template will be loaded, unless a template with the same name is found in the project's templates directory, since the filesystem loader is listed first in the TEMPLATE_LOADERS setting. This allows a project developer to easily override the templates for a third-party app by copying it into their project's templates directory, in order to customise the look and feel of the app.\n\nA problem arises for the project developer however, when the app's template contains sufficiently complex features, like many extendable template blocks, template variables, and more. Once they copy the template to their project's templates directory, they're essentially forking it, that is, they'll no longer be able to seamlessly make use of any new features for that template in future versions of the third-party app. Worst case is that an upgrade to the app will break their project, until they copy the new version of the template and customize it again, or upgrade their own copy by hand to be compatible with the latest version of the app.\n\nWith a complex template like this, more often than not the project developer may simply want to change it in a very small way, such as modifying the content in one of its blocks. Wouldn't it be nice if you could use template inheritance to extend the app's template, and simply override the relevant blocks as desired? Unfortunately this isn't possible with Django due to circular template inheritance. The app's view will be looking for the template to load by a given name. If we want our project's version to be used, we need to use the same template name for it to be loaded. If our project's version of the template tries to extend a template with the same name, Django will load our project's template again when looking for the parent template to extend, resulting in an infinite loop that will never complete. Django's template inheritance isn't smart (or stupid) enough to ignore the absolute path of the current template being used, when searching for the parent template to extend.\n\nAlternative Approaches\n\nThe general approach to dealing with this problem, is for app developers to separate the name of of the template being loaded by their view, from the parts of the template that a project developer may want to customise. This might involved breaking all of the features up into separate include files that can be overridden individually. Another approach is to make each view load an empty template that extends the real template - developers can then override the empty template, end\nextend the real template as required.\n\nIn an effort to make Mezzanine's templates more customisable, these ideas were recently brainstormed on the Mezzanine mailing list. While these approaches might work extremely well for individual Django apps that only provide a handful of default templates, the question of complexity and maintenance comes up with larger-scale projects like Mezzanine which contains almost 100 template files at the moment. All of a sudden we're looking at a minimum of doubling the number of template files - even more if we get more granular with includes. We've lost the simplicity of simply checking which template a view loads, and copying it to our project for customisation. So the question was proposed as to how could we possibly get circular template inheritance to work - if that was possible, we'd have a fantastic tool for both overriding and extending templates at once, without any wide-sweeping changes to the template structure across the entire project. Read on for the gory details of how it's quite possible.\n\nHacks Inside\n\nFair warning: the rest of this post describes an unorthodox approach that allows circular template inheritance to work. It's a little bit crazy, it's a little bit cool. Some would call it a terrible hack, your mileage may vary. If you're going to use it, consider the actual problem it solves, and whether or not it applies to your situation. Understand what it does, test it, and weigh it up against the alternative approaches described earlier.\n\nThe problem can be boiled down to one of state - when Django's extends template tag is used and the parent template to extend is searched for, the tag contains no knowledge of the extending template calling it. If it did, we could theoretically exclude its path from those being used to search for the parent template. So a possible solution might involve two steps:\n\n\n  Make the extends tag aware of the path for the template that's calling it.\n  Search for a template with the same name, preferably using Django's template loaders, and exclude the path of the template doing the extending.\n\n\nBoth of these steps are achievable thanks to the design of Django's template loaders. While the underlying location of a template isn't available through the higher level interfaces like get_template and find_template, if you dig down slightly into the template API, you'll find each loader contains a load_template_source method, which is part of the call chain for finding and loading a template's source. load_template_source returns both the source for the template it discovers, as well as the absolute path of the template.\n\nFrom this point we could go ahead and fulfill the second step by searching all other possible file-system paths for a template with the same relative template path, excluding the absolute path we've retrieved via load_template_source, but ideally we'd like to leverage the template loaders to do this. Fortunately this is a breeze given the way template loaders work. The load_template_source method for each of the template loaders can accept a list of directories to use, and will fall back to a default if none are specified. The filesystem loader will use the directories defined by the TEMPLATE_DIRS setting, and the app_directories loader will use a list of template directories for all of the INSTALLED_APPS which it builds up when first loaded. I've never seen these directory arguments used in practice, but there they are just begging to be exploited as the perfect solution to our template searching problem.\n\nExtending Extends\n\nNow that we have the theoretical hooks needed, it's time to implement our template tag. Again we find ourselves in the situation where the pieces of Django we need to touch are structured perfectly to do what we need. The extends tag is implemented using the ExtendsNode class. It contains a get_parent method, which is responsible for loading the parent template object that is being extended. So all we need to do is subclass ExtendsNode and override get_parent. We'll also include our own find_template method, similar to Django's, that also returns the absolute path of the template that was found. I've dubbed this approach 'overextending', since it allows you to both override and extend a template at the same time.\n\nfrom django.conf import settings\nfrom django.template import TemplateDoesNotExist\nfrom django.template.loader_tags import ExtendsNode\n\nclass OverExtendsNode(ExtendsNode):\n\n    def find_template(self, name, dirs):\n        \"\"\"\n        Replacement for Django's ``find_template``, that also returns\n        the location of the template found.\n        \"\"\"\n        for loader_name in settings.TEMPLATE_LOADERS:\n            loader = find_template_loader(loader_name)\n            try:\n                source, path = loader.load_template_source(name, dirs)\n            except TemplateDoesNotExist:\n                pass\n            else:\n                return source, path\n        raise TemplateDoesNotExist(name)\n\n    def get_parent(self, context):\n        \"\"\"\n        Load the parent template using our own ``find_template``, which\n        will also give us the path of the template found. We then peek\n        at the first node, and if its parent arg is the same as the\n        current parent arg, we know circular inheritance is going to\n        occur, in which case we try and find the template again, with\n        the absolute directory removed from the search list.\n        \"\"\"\n        from django.template.loaders.app_directories import app_template_dirs\n        dirs = list(settings.TEMPLATE_DIRS + app_template_dirs)\n        parent = self.parent_name.resolve(context)\n        template, path = self.find_template(parent, dirs)\n        if (isinstance(template.nodelist[0], ExtendsNode) and\n            template.nodelist[0].parent_name.resolve(context) == parent):\n            # Remove the template directory from the available\n            # directories to search in, and try again.\n            dirs.remove(path[:-len(parent) - 1])\n            template, path = self.find_template(parent, dirs)\n        return template\n\n\n\nAll that remains is creating the overextends template tag function that uses OverExtendsNode. For this we can pretty much copy pasta Django's extends tag function, replacing ExtendsNode with OverExtendsNode.\n\nDiving Deeper - Unlimited Inheritance Levels\n\nKeeping in mind that this approach is is specifically geared towards solving the problem of both overriding and extending a template in a third party app, that is, one we don't want to modify the source code of, our approach so far works. But what if we wanted to overextend a template that also overextends another template? Say for example our project template overextends a template in third-party app 'A', which is dependent on a template in third-party 'B', that it also overextends. This is quite an edge case, but the code above would fail in this scenario. When the template in app 'A' tries to overextend the template in app 'B', it would exclude itself from the search path, and end up loading our project's version of the template, and we're back to square one with circular inheritance never completing.\n\nThis is less of an edge case and much more likely, with frameworks like Mezzanine, where it's common for people to create themes as third-party apps. The theme provides a set of templates and static files, and gets added to INSTALLED_APPS just as a regular Django app would. With the approach of overextending introduced into the eco-system, theme developers may overextend Mezzanine's templates, and project developers may overextend the theme's templates. Taking this into account, our approach so far looks quite handicapped.\n\nAgain we have a state problem - the second and subsequent calls to overextends have no knowledge of the previous calls, so they can't exclude the chain of template directories that have been so far excluded when overextending.\n\nWe can solve this problem by making use of the template context to store the state of directories excluded so far when using overextends. We store a dictionary mapping template names to lists of directories available. In the code above, we build the full list of directories to use each time overextends is called. If we maintain that list in the template context, removing from it each time overextends is used, we can support unlimited levels of circular template inheritance.\n\nHere's an refactored version of the previous example, along with the overextends tag function, supporting multiple levels of circular template inheritance.\n\nfrom django import template\nfrom django.template import Template, TemplateSyntaxError, TemplateDoesNotExist\nfrom django.template.loader_tags import ExtendsNode\nfrom django.template.loader import find_template_loader\n\nregister = template.Library()\n\nclass OverExtendsNode(ExtendsNode):\n\n   def find_template(self, name, context, peeking=False):\n        \"\"\"\n        Replacement for Django's ``find_template`` that uses the current\n        template context to keep track of which template directories it\n        has used when finding a template. This allows multiple templates\n        with the same relative name/path to be discovered, so that\n        circular template inheritance can occur.\n        \"\"\"\n\n        # These imports want settings, which aren't available when this\n        # module is imported to ``add_to_builtins``, so do them here.\n        from django.template.loaders.app_directories import app_template_dirs\n        from django.conf import settings\n\n        # Store a dictionary in the template context mapping template\n        # names to the lists of template directories available to\n        # search for that template. Each time a template is loaded, its\n        # origin directory is removed from its directories list.\n        context_name = \"OVEREXTENDS_DIRS\"\n        if context_name not in context:\n            context[context_name] = {}\n        if name not in context[context_name]:\n            all_dirs = list(settings.TEMPLATE_DIRS + app_template_dirs)\n            context[context_name][name] = all_dirs\n\n        # Build a list of template loaders to use. For loaders that wrap\n        # other loaders like the ``cached`` template loader, unwind its\n        # internal loaders and add those instead.\n        loaders = []\n        for loader_name in settings.TEMPLATE_LOADERS:\n            loader = find_template_loader(loader_name)\n            loaders.extend(getattr(loader, \"loaders\", [loader]))\n\n        # Go through the loaders and try to find the template. When\n        # found, removed its absolute path from the context dict so\n        # that it won't be used again when the same relative name/path\n        # is requested.\n        for loader in loaders:\n            dirs = context[context_name][name]\n            try:\n                source, path = loader.load_template_source(name, dirs)\n            except TemplateDoesNotExist:\n                pass\n            else:\n                # Only remove the absolute path for the initial call in\n                # get_parent, and not when we're peeking during the\n                # second call.\n                if not peeking:\n                    context[context_name][name].remove(path[:-len(name) - 1])\n                return Template(source)\n        raise TemplateDoesNotExist(name)\n\n    def get_parent(self, context):\n        \"\"\"\n        Load the parent template using our own ``find_template``, which\n        will cause its absolute path to not be used again. Then peek at\n        the first node, and if its parent arg is the same as the\n        current parent arg, we know circular inheritance is going to\n        occur, in which case we try and find the template again, with\n        the absolute directory removed from the search list.\n        \"\"\"\n        parent = self.parent_name.resolve(context)\n        # If parent is a template object, just return it.\n        if hasattr(parent, \"render\"):\n            return parent\n        template = self.find_template(parent, context)\n        if (isinstance(template.nodelist[0], ExtendsNode) and\n            template.nodelist[0].parent_name.resolve(context) == parent):\n            return self.find_template(parent, context, peeking=True)\n        return template\n\n@register.tag\ndef overextends(parser, token):\n    \"\"\"\n    Extended version of Django's ``extends`` tag that allows circular\n    inheritance to occur, eg a template can both be overridden and\n    extended at once.\n    \"\"\"\n    bits = token.split_contents()\n    if len(bits) != 2:\n        raise TemplateSyntaxError(\"'%s' takes one argument\" % bits[0])\n    parent_name = parser.compile_filter(bits[1])\n    nodelist = parser.parse()\n    if nodelist.get_nodes_by_type(ExtendsNode):\n        raise TemplateSyntaxError(\"'%s' cannot appear more than once \"\n                                  \"in the same template\" % bits[0])\n    return OverExtendsNode(nodelist, parent_name, None)\n\n\n\nThe final step required is to automatically add our overextends tag to Django's built-in tags. Django's ExtendsNode uses a feature where it gets marked as having to be the first tag in a template (ExtendsNode.must_be_first is set to True). This means that it (and subsequently our ExtendsNode subclass) need to be available without having to load the template library that implements it. This is as simple as calling the django.template.loader.add_to_builtins function from your project's settings module, passing it the Python dotted path as a string for the module that contains out overextends tag.\n\ndjango-overextends\n\nOriginally this post contained a similar approach to the one above, but made use of the origin attribute found on template objects. Shortly after publishing it, Tobia Conforto helped me work out that the origin attributes on template objects are only available when DEBUG is True in your Django project. A big thanks goes out to him for bringing this up, allowing me to work out a more solid approach that this post now describes.\n\nTobia also suggested how badly a solution like this is needed for reusable app developers, and that it really belongs in a publicly available package that people can install and add to their Django projects. So I've bundled it into a reusable Django application called django-overextends. It contains documentation, tests, continuous integration, and is available on PyPI, GitHub and Bitbucket.", "published": 0, "publish_date": "2012-05-17T17:00:00", "slug": "circular-template-inheritance-for-django", "title": "Circular Template Inheritance for Django"}}, {"pk": 5, "model": "blog.blogpost", "fields": {"content": "The actual code written when developing a typical web application, ranks pretty lowly on the complexity scale, when compared to the rest of the software engineering industry. Of course, web development has its own set of interesting complexities around architecture and scaling, but the application code itself is relatively simple. Glue one library to another, update a row in a database, print some more rows out, send out an email, and so on.\n\nAs a web developer, I always enjoy the chance to dig deeper into more formal data structures and algorithms when the opportunity arises. At Fairfax I've recently had two distinct situations come up where I've needed to work through sets of data, with each of the items in the set containing references to other items that they depends on. The desired outcome was to sort the items so that I could process each item, knowing that each time I reached an item with dependencies, those dependent items would already have been processed.\n\nThe first scenario was one with serialized Django models. I had a number of model instances that would be instantiated and serialized, with the aim of persisting them to the database at a later point. These models contained relationships with other models that were also being serialized. Temporary primary keys were used in order to build their relationships, allowing all of the models to then be serialized with their relationships intact.\n\nThe problem then arose after deserializing these instances, and persisting them to the database. Each model with a foreign key to another couldn't be processed, until the model it was related to had been persisted to the database first, as real primary keys are required for foreign key relationships to be defined. My first solution was a quick hack that manually sorted the objects correctly, knowing in advance which classes of models I was dealing with. This of course meant the code couldn't be reused in a general manner, and as more classes of models were added to the mix down the track, this would need to be addressed.\n\nThe second case like this came up in an entirely different situation. At Fairfax we're building an impressive distributed system built on many RESTful APIs. Each of these APIs contains its own schema, and we set out to build a tool that could introspect these schemas, and generate client objects for interacting with them. These schemas also contain relationships between resources, and these resource relationships are also modelled in our client objects. Again this situation was one where I needed to process an unordered set of resources in the correct order, so that their relationships could correctly reference resources already handled.\n\nShyly fool my bitten shame twice, or something like that. When this problem came around the second time, I didn't have the luxury of knowing up front what the exact classes of data I'd be dealing with were, as they were due to change quite quickly as we iterated. I then set out to implement the correct solution for the problem, that could be applied to both of these situations.\n\nA quick Google search for 'dependency resolution' brought me to the Wikipedia page for topological sorting, which was the solution I was looking for. In both cases, we have graphs. Think of a graph as points on a map called nodes, with each node connected by lines called edges, to other nodes in the graph.\n\nA simple graph\n\nA directed graph is one where each of the edges contain a direction, so each of the lines contain an arrow pointing one way or the other. An acyclic directed graph is where each of the edges only point in one direction, so that it's not possible to follow the edges from one node to another, returning back to the original node. If this last condition is not satisfied, then the graph is said to contain directed cycles, that is, a path can be followed from one node to others, and back to the original node again.\n\nAn acyclic directed graph\n\nIn both scenarios I faced, my graph structure was different from those commonly found in examples of topological sorting. I had the outgoing edges defined, rather than incoming edges. The gist of the topological sort I needed, is to repeatedly go through all of the nodes in the graph, moving each of the nodes that has all of its edges resolved, onto a sequence that forms our sorted graph. A node has all of its edges resolved and can be moved, once all the nodes its edges point to, have been moved from the unsorted graph onto the sorted one.\n\nConsider the graph above from left to right, as pairs of nodes and their outgoing edges:\n\ngraph_unsorted = [(2, []),\n                  (5, [11]),\n                  (11, [2, 9, 10]),\n                  (7, [11, 8]),\n                  (9, []),\n                  (10, []),\n                  (8, [9]),\n                  (3, [10, 8])]\n\n\n\nOur expected output from a topological sort function would be as follows, with no nodes containing edges pointing to nodes before themselves:\n\n&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; pprint(topolgical_sort(graph_unsorted))\n[(2, []),\n (9, []),\n (10, []),\n (11, [2, 9, 10]),\n (5, [11]),\n (8, [9]),\n (3, [10, 8]),\n (7, [11, 8])]\n \n\n\n\nNote that the ordering need not be precisely the same each time. In the result above, node 9 could come before node 2, as neither of these contain edges, so they equally belong first in the sorted graph.\n\nHere's an implementation of my topological sort in Python:\n\ndef topolgical_sort(graph_unsorted):\n    \"\"\"\n    Repeatedly go through all of the nodes in the graph, moving each of\n    the nodes that has all its edges resolved, onto a sequence that\n    forms our sorted graph. A node has all of its edges resolved and\n    can be moved once all the nodes its edges point to, have been moved\n    from the unsorted graph onto the sorted one.\n    \"\"\"\n\n    # This is the list we'll return, that stores each node/edges pair\n    # in topological order.\n    graph_sorted = []\n\n    # Convert the unsorted graph into a hash table. This gives us\n    # constant-time lookup for checking if edges are unresolved, and\n    # for removing nodes from the unsorted graph.\n    graph_unsorted = dict(graph_unsorted)\n\n    # Run until the unsorted graph is empty.\n    while graph_unsorted:\n\n        # Go through each of the node/edges pairs in the unsorted\n        # graph. If a set of edges doesn't contain any nodes that\n        # haven't been resolved, that is, that are still in the\n        # unsorted graph, remove the pair from the unsorted graph,\n        # and append it to the sorted graph. Note here that by using\n        # using the items() method for iterating, a copy of the\n        # unsorted graph is used, allowing us to modify the unsorted\n        # graph as we move through it. We also keep a flag for\n        # checking that that graph is acyclic, which is true if any\n        # nodes are resolved during each pass through the graph. If\n        # not, we need to bail out as the graph therefore can't be\n        # sorted.\n        acyclic = False\n        for node, edges in graph_unsorted.items():\n            for edge in edges:\n                if edge in graph_unsorted:\n                    break\n            else:\n                acyclic = True\n                del graph_unsorted[node]\n                graph_sorted.append((node, edges))\n\n        if not acyclic:\n            # Uh oh, we've passed through all the unsorted nodes and\n            # weren't able to resolve any of them, which means there\n            # are nodes with cyclic edges that will never be resolved,\n            # so we bail out with an error.\n            raise RuntimeError(\"A cyclic dependency occurred\")\n\n    return graph_sorted\n\n\n\nI imagine some other uses for topological sorting would be task queues, where certain tasks are dependent on other tasks being completed first. It could also be used by package managers that install software libraries, to ensure each library has its dependencies met before it's installed.", "published": 1, "publish_date": "2012-04-06T17:00:00", "slug": "topological-sorting-acyclic-directed-graphs", "title": "Topological Sorting Acyclic Directed Graphs"}}, {"pk": 6, "model": "blog.blogpost", "fields": {"content": "This weekend I finally launched Mezzanine 1.0 after two years in development.\n\nRather than talk about Mezzanine itself and the lead up to 1.0, I thought it would be fun to look at the online reach of the release announcement, as well as some of the growth that has occurred over the last 18 months since I first made Mezzanine available. For more detail on Mezzanine and the release itself, check out the 1.0 announcement to the wider Django community, as well as the discussion leading up to the 1.0 release.\n\nActive Growth\n\nAt the end of 2010 I wrote my annual year in review post, where I talked about Mezzanine and the activity that had occurred in its first 6 months. With the new 1.0 release, I thought it would be interesting to take a fresh look at some of the stats I talked about back then. We can see below that growth has continued all the way through at a steady pace.\n\n\n\n    &nbsp;\n    Version 0.9.16 months\n    Version 1.0.02 years\n\n\n    Project followers\n    120\n    430\n\n\n    Project forks\n    30\n    130\n\n\n    Project contributors\n    10\n    40\n\n\n    Mailing list subscribers\n    60\n    190\n\n\n    Mailing list messages\n    300\n    1,800\n\n\n    PyPI downloads\n    4,000\n    38,000\n\n\n    Homepage visitors\n    9,000\n    47,000\n\n\n\nCombined follower/fork count for GitHub and Bitbucket brought to you by One True Repo\n\nThe Launch Party\n\nThe launch party we held for the release was a raging success. Just to be clear, by we I mean me, and by party I mean sitting at my computer all day with several browser tabs open, feverishly refreshing them with the hope of some good exposure and interest around the release. And there was plenty!\n\nAfter first announcing the release to the django-users mailing list, I then posted it to a handful of popular channels in the programming community. All around the responses were positive, ranging from congratulation and praise, to all sorts of questions from curious people in the Django space who hadn't heard of Mezzanine before.\n\ndjango-users\n\nAs mentioned, I made the initial announcement to the django-users mailing list. Here I gave a general overview of Mezzanine and Cartridge, and went through their core features. The follow-up responses were positive, and included some questions about how Cartridge compares to other more popular ecommerce Django apps. I then gave a short version of my original post from when I first made Cartridge available, which covers that area in more detail.\n\nHacker News\n\nI've posted a few articles to Hacker News before, but they've never been promoted to the front page, which occurs after receiving enough votes from the community. So I was delighted when 10 minutes or so after posting the announcement to Hacker News, it had reached enough votes to hit the front page. After that the votes came pouring through, with the announcement eventually making its way to 4th place on the front page, where it remained for most of the day.\n\n\n\nWhen I woke up the next day, the article had reached over 90 votes, and a good range of questions had been posted, from looking for help getting started, to comparisons against other CMS projects. By then the news of the latest Rails exploit had flooded the front page, and the Mezzanine release had been pushed down into oblivion.\n\n\n\nReddit\n\nAt the same time, I posted the announcement to Reddit. It also reached the front page of proggit, the programming sub-reddit I posted it to. Again there were a good number of comments with praise and questions. What was interesting about Reddit is that the number of votes both for and against the article are visibly displayed.\n\nDownvotes, really!!?\n\nIt was amusing to think about what would cause people to vote it down. Perhaps they were fanatical supporters of another language, framework or CMS. I'll never know! I think it's a safe bet though that it had nothing to do with Mezzanine itself, based on the actual comments posted there and elsewhere.\n\nTwitter\n\nOf course I tweeted the announcement once it was made. After a handful of direct and indirect retweets, the tweets started coming through with a lot of lovely praise, particularly for the responsive layout of Mezzanine's new project homepage, something that all credit goes to the Bootstrap team for. The most humbling moment though was when a tweet came through from Antonio Rodriguez, the former CTO of HP of all people!\n\n\n  This looks like it may be Django's killer app\n\n\nVery cool.\n\nAfter the announcement had hit the front pages of Hacker News and proggit, an army of Twitter bots that are connected to those sites then tweeted links to the announcement, resulting in hundreds of tweets and thousands of new visitors. The final assault was then triggered by Smashing Magazine, who tweeted Mezzanine to their half a million followers.\n\nGitHub\n\nWhile GitHub wasn't somewhere I explicitly made the announcement to, like the channels above, the reaction on there was probably the most important. Mezzanine and Cartridge received over 100 new developers following the projects.\n\nThis resulted in Mezzanine being the most watched Python project on GitHub for both the day and the week! For the first time it also entered into the top 100 Python projects on GitHub of all time.\n\n\n\nA quick burst of small contributions followed, from spelling corrections in the documentation, to patches for getting things running smoothly on Windows. I then released version 1.0.1, and 1.0 became history.\n\nFinal Toast\n\nIt's been a hard slog over the last two years. With a full-time job and a family to take care of, the biggest challenge has always been finding the time to build new features and work with the community to ensure Mezzanine stays on the right track for its users. In that regard, Mezzanine wouldn't be what it is today without the contribution of all the developers who have written features, fixed bugs, and most importantly, helped out new-comers on the mailing list. A special thanks goes out to all of you:\n\n\n\nLex Hider\nVan Lindberg\nTimur Bobrus\nToby White\nEric Floehr\nTom von Schwerdtner\nBrad Montgomery\nAndrew Fisher\nCarlos David Marrero\nLee Matos\nJosh de Blank\nDominique Guardiola Falco\nMicha\u0142 Oleniec\nJohn Campbell\nAndrew Grigorev\nAudrey Roy\nJosh Cartmell\nOsiloke Emoekpere\nEduardo Gutierrez\nRich Atkinson\nBrett Clouser\nBrent Hoover\nOwen Nelson\nZeke Harris\nKen Bolton\nEli Spizzichino\nMichael Delaney\nDavid Prusaczyk\nAlexey Makarenya\nSebasti\u00e1n Magr\u00ed\nKevin Levenstein\nJosh Batchelor\nJohn Barham\nLuke Plant\nZden\u011bk Softi\u010d\nAlvin Mites\nJason Kowaleski\nNicola Larosa\nAnders Hofstee\nChris Trengove\nChris Smith\nTommy Wolber\n\nHere's looking forward to the next two years, and keeping Mezzanine and Cartridge a lean, mean, site building machine.", "published": 1, "publish_date": "2012-03-05T18:00:00", "slug": "mezzanine-10-the-aftermath", "title": "Mezzanine 1.0: The Aftermath"}}, {"pk": 7, "model": "blog.blogpost", "fields": {"content": "It was almost a year ago that I took up a new role using Ruby on Rails. I've previously talked about my thoughts on Rails, and given my experience with Django I probably wouldn't consider using Rails for my own projects. What I did explore in my time with Ruby was another framework called Sinatra which I used to build several apps. Firstly I'll go over Sinatra and some of the related pieces in the stack, and then I'll cover the apps I built.\n\nSinatra\n\nSinatra is a micro-framework, which differs from mega-frameworks like Rails and Django, in that Sinatra is bare-bones. It mostly deals with mapping URLs to request handlers, and not much more beyond that. No templating, no ORM, no middleware. All of these features can be slotted in using third party libraries where required. This makes for a very pleasant development experience with smaller sized apps - instead of having to do everything the Django/Rails way, you're free to pick and choose the parts you need, and weave them together in the best way you see fit. You're working at a relatively lower level, with much less scaffolding, and a lot more flexibility and control.\n\nPython has its own counterparts in this space as well, such as Bottle and Flask. However at the time I was looking to dive further into Ruby, and Sinatra seemed like a great way to lean into it.\n\nIf you're new to web development, or an experienced developer coming to Ruby or Python from older stacks like ASP.NET or PHP, I'd highly recommend starting out with a micro framework like Sinatra or Flask, before moving onto their bigger siblings Rails and Django. You'll get a great feel for their respective languages, without getting bogged down in the frameworks themselves.\n\nDataMapper\n\nYou can't go very far these days developing a web application, without needing some form of persistent storage such as a database, and a library to work with it that goes beyond hand-written SQL. Django has its own ORM which is very powerful, but suffers from lacking a blessed, seamless migration tool. Rails has Active Record, which has grown into the defacto ORM in the Ruby eco-system, and has its own set of problems. The main issue I had with Active Record was that there was no clear definition of what fields a particular model implemented, aside from diving directly into the database itself. It coincides clearly with the notion that Rails contains too much magic. Compare this to Django's declarative ORM, where each model's class contains an explicit blueprint of which fields and methods the model implements. The value of this in quickly picking up a new code base is highly under-stated, if the popularity of Active Record is anything to go by.\n\nI began looking for an ORM solution for Sinatra and quickly came across a project called DataMapper, and was incredibly pleased. Not only does it provide the same declarative style that Django's ORM does, it goes above and beyond that with features that blow both Django's ORM and Active Record out of the water.\n\nFirstly it provides the ability to automatically migrate models. Arguments against magic aside, this is an amazing feature. Simply change the model, and the changes are migrated to the database when next instantiated.\n\nSecondly, DataMapper completely eliminates the N+1 query problem. When iterating through data and accessing other models via related fields, DataMapper will query the database when the first relationship is accessed, retrieving all required data pertaining to the outer loop, building up all of the instance relationships on the fly prior to accessing them. Yes, it sees into the future and protects you from obliterating your database, a mistake all too common in web development. It's worth noting that Django introduced this feature in the yet to be released 1.4 with the prefetch_related method. This is another great example of the explicitness of Python compared to the implicitness of Ruby.\n\nWhether you're building a web application or not, if you're accessing a database from Ruby, consider using DataMapper. It's a great piece of software.\n\nHeroku\n\nThe explosion in Ruby and Python development frameworks over the last half decade has been a boon to web development. Security, modularity, shelf life, and time to market have all dramatically improved thanks to dynamic languages, the frameworks that have developed around them, and the open source communities that make them possible. It's not all fun and games however. Deployment of these applications has grown considerably more complex. Gone are the days of using FTP to upload some PHP scripts to a server, and hitting refresh on the web page to test your changes. We now have to deal with a wide variety of deployment tasks, from reloading application processes, database migrations, dependency management and much more.\n\nNaturally the community has risen to solve these problems, a movement sometimes referred to as DevOps, with tools in place such as Ruby's Capistrano, Python's Fabric, and configuration management tools to map complex deployments such as Chef and Puppet. While the learning curve is steep, with enough time invested up front, deployments can become as simple as pushing a button, and are more robust and integrated with quality assurance than ever before.\n\nModern deployments such as these require experts. This is where Platform as a Service (PaaS) offerings come in. PaaS providers are modern hosting companies that take care of all the dirty work in configuring servers and automating deployments for you. Typically they'll expose a distributed version control server using Git or Mercurial that you can push code commits to. The process of pushing commits then triggers deployment, automatically performing all of the required tasks. PaaS providers will also provide all of the related services required, such as databases, message queues, caching servers and so forth.\n\nAre PaaS providers a magic bullet? Absolutely not. Every application will have a tipping point where it grows beyond the 'one size fits all' approach provided by PaaS offerings. You're also at the mercy of the provider when it comes to uptime, so mission critical applications with strict service level agreements may require much more fine grained control in their hosting environments. Never forget that you are the only one responsible for your service's availability. However for smaller, stock, non-critical applications, PaaS providers are a dream come true, that remove most of the complexities around service provisioning, configuration and application deployment.\n\nThe most well known of these providers is Heroku, who were the first to popularise the PaaS architecture. Not only does Heroku offer a rich variety of add-on services that I was looking for, like Varnish, Memcached, and PostgreSQL, they also provide free hosting for low capacity sites - ideal for the types of applications I ended up building with Sinatra.\n\nSo with Sinatra, DataMapper and Heroku combined, I developed several small applications that scratched particular itches for me, in order to build up a good working knowledge of Ruby.\n\nLinked Out\n\nI don't keep an up to date CV anymore. If I pick up a new skill, or start a new role, I'll update my LinkedIn profile. It's the quickest and easiest way to keep my professional information up to date. For better or worse though, over the last few years LinkedIn has turned into a mass hunting ground for recruiters. I used take the time to enter into a dialogue with each and every recruiter that contacted me, after all anything less would be rude, but over time I realised the futility in this, as the practice by recruiters  to blast out boilerplate introductions to anyone who matched a keyword search, became more and more common. But I digress. The state of recruitment aside, these conversations would inevitably lead to recruiters asking for a CV they could present to their clients. LinkedIn profiles contain a 'download as PDF' feature, which I would always refer recruiters to, but LinkedIn embeds their logo within the PDF, and over time as they've added new profile features, the PDF download hasn't picked these up. What I'd always wanted was an easy way to export my profile as a clean PDF, containing only the information relevant to a CV. I also wanted to be able to share the tools with anyone else who wanted to use it, so a baby Sinatra app seemed like the perfect fit.\n\nThere's a great Ruby library for interacting with LinkedIn's API, and PDFKit for converting HTML to PDF, which meant that I could format the CV using HTML and CSS, and all I then needed to do was convert that directly to PDF.\n\nThe app I ended up building is called Linked Out. It has a very simple flow to it. You first authenticate via LinkedIn's oAuth service, and then you're redirected back to Linked Out, where you can create a PDF version of your profile. As an added bonus I hooked into LinkedIn's connections API, so you also get the option of creating a CV for any of your LinkedIn connections - no need for recruiters to bother people with CV requests, they can create the CVs themselves.\n\nLinkedIn is somewhat lacking when it comes to formatting large blobs of text in profiles. People tend to create all sorts of formatting themselves, typically to create bulleted lists. So Linked Out contains some smarts to look for these different types of free-text formats, converting them into proper lists and headings where appropriate.\n\nNeed to fend off pesky recruiters with a nice looking CV? Go and update your LinkedIn profile, and give Linked Out a try.\n\nKlout Feed\n\nIf you're unfamiliar with Klout, it's a reputation measurement system that gives you a daily score based on your online interactions. It looks at your Twitter account as well as other social media services, and applies an algorithm based on the number of mentions you receive, retweets, and favourites, also taking into account the Klout score for each of the people who trigger these. It then assigns you a score out of 100 which you can measure on a daily basis to gauge how effectively you're using Twitter. According to Klout at least.\n\nKlout has been described as many things, from a revolutionary game-changer in social media, to a vapid and narcissistic waste of time. Personally I found it to be an entertaining distraction. It did keep me coming back each day to check my score.\n\nNow I'm one of a dying breed that still uses RSS exclusively to keep track of everything going on online. Updates from my LinkedIn connections, contributions and issues for my projects on GitHub, Google Groups mailing lists, and of course various news sites and blogs. They all contain RSS feeds which I can catch up on, in one single interface. But not Klout. You need to log into their site each time you want to check you score. There's an interesting point here. If you want to keep users coming back to your site and piss them off at the same time, create a great service but make sure you don't include an RSS feed for the data your users are interested in.\n\nTo solve this I put together an app called Klout Feed. It uses Klout's API to provide an RSS feed for each user, publishing their score and its change each day. The flow isn't as seamless as Linked Out's. Klout doesn't provide any form of application integration the same way LinkedIn, Twitter and Facebook do. Just a per-user API key with daily limits assigned to it. So with Klout Feed you first need to head on over to Klout and grab an API key, then bring that back to Klout Feed to get the URL for your score's RSS feed.\n\nIan Anderson has since gone ahead and written a how-to article on combining Klout Feed with the If This Then That service. The result is that you can get email or SMS notifications each time your Klout score changes.\n\nOne True Repo\n\nMost open source projects use Git or Mercurial for version control, and are hosted on either GitHub or Bitbucket respectively. Some people like myself host their projects on both sites. I've talked about my setup for hosting on GitHub and Bitbucket before. Both sites provide totals for the number of interested developers following the project, and the number who have forked the project. A fork is when someone creates a copy of a project, usually with the intent of adding some news features or fixes, and pushing them back to the original source.\n\nWhat I've always wanted is a combined API for totaling followers and forks across both services for a single project hosted on both sites, and also for all projects for a given user on both sites. This is what I tackled for my next Ruby project, which I called One True Repo (OTR).\n\nOTR's original form was as a library that other developers could embed in their project, so I built it as a Ruby gem that you can both include in your own project, or simply run from the command line and pipe the data it returns into other programs. The next step was to build a baby Sinatra app that provided a hosted version of the API that people could query. The project itself contains everything for all three of these forms - the library, the command-line tool, and the Sinatra web app.\n\nQuerying the GitHub API was trivial and all the information I wanted was provided by it very easily. The Bitbucket API wasn't quite up to scratch for this however. Remarkably it doesn't expose follower and fork count on each project. Some screen-scraping was therefore required to get these totals for each of a user's projects.\n\nDo you mirror your open source projects across both GitHub and Bitbucket? Ever wonder how many people are following all your projects on both services? Give One True Repo a try.\n\nConclusion\n\nAs you can see from some of these apps, the Sinatra on Heroku combination is the perfect fit for small mashups that act as glue between other popular APIs. Free of charge, rapid development, and a great pool of libraries to choose from in the Ruby eco-system.", "published": 0, "publish_date": "2012-02-27T18:00:00", "slug": "my-baby-sinatra-apps", "title": "My Baby Sinatra Apps"}}, {"pk": 8, "model": "blog.blogpost", "fields": {"content": "Another year has gone by and another obligatory year in review post is due. 2011 was a\nyear full of change for me. I changed jobs twice, spent half the year coding in a new language, moved interstate, and switched my primary operating system.\n\nRuby\n\nAfter working at Citrus for almost nine years, I was well overdue for a change of scenery, so when I was approached by Impact Data to come and work for them using Ruby on Rails, I welcomed the opportunity. My time there was short and sweet however, as after six months I decided to move back home to Sydney, but it was a really rewarding experience working with an incredibly smart team on a technology stack that was new to me. I had a great time learning Ruby, forming a strong appreciation for its elegance, and it was very interesting along the way making plenty of comparisons between Rails and Django. Naturally I started using Ruby in my own projects, getting to know Sinatra for a handful of apps that I built. I'll be writing a more detailed post about those soon, so stay tuned!\n\nDjango Dash\n\nFor the second year in a row I entered the Django Dash hackathon. This time around I wanted to do something that really pushed Django outside of its typical usage patterns. I'd recently read Cody Soyland's introductory blog post on using WebSockets with Django, and so I came up with an idea I called Drawn By, a collaborative drawing app where people could create sketches together in real-time, save them to an image they can download, and rate others' sketches in the gallery.\n\nI got to use a variety of technology I hadn't used before which was really fun. I used gevent as the evented web server for maintaining open socket connections with the browser, the NoSQL database Redis for queuing events and storing temporary pixel data, and the browser's Canvas API for front-end drawing interaction and rendering. I set a relatively high goal for ourselves this year with what we wanted to achieve, but we pulled it off nicely in the end.\n\nThis year we came 3rd place out of around 50 entries, which was a great improvement on the previous year's result of 8th. The most important result however was the creation of django-socketio, which was extracted from Drawn By and released as open source. It brings together all of the scaffolding for using WebSockets with Django, and implements an events and channels system for building your own applications around it. I previously wrote about django-socketio right after releasing it, and since then it has gained quite a lot of traction, with a handful of developers contributing back fixes and new features.\n\nOpen Source\n\nI actually spent less time this year contributing to open source than I did the previous year, but I steamed ahead nonetheless with a lot of new projects, as well as continued development and support for my major works, Mezzanine and Cartridge. Both these projects have reached a very mature level over the course of 2011, thanks to tons of contributions from the Mezzanine and Cartridge community, which continues to grow steadily. Here's a list of the projects I released as open source over the year:\n\n\n  Drawn By: Collaborative real-time sketching. (Django / Python)\n  django-socketio: WebSockets for Django. (Django / Python)\n  Virtualboxing: Comparison utilities for Riak and MongoDB. (Ruby)\n  Grillode: Multi-purpose chat server. (Node / CoffeeScript)\n  Linked Out: Create PDF resumes for LinkedIn. (Sinatra / Ruby)\n  Klout Feed: Daily Klout scores via RSS. (Sinatra / Ruby)\n  Babbler: A Twitter bot. (Python)\n  One True Repo: Combined GitHub and Bitbucket API. (Sinatra / Ruby)\n  hg-github: A Mercurial extension for GitHub. (Mercurial / Python)\n  sphinx-me: A Sphinx documentation generator. (Python)\n\n\nOSX\n\nAs I mentioned, towards the end of the year I moved back to Sydney. An opportunity came up to work with Fairfax, the largest media organisation in Australia. Fairfax is building a new publishing platform using Django, so my experience with content management and Django was a natural fit. Mostly though, it was a chance for me to move back home and be closer to my family, after being away from them in Melbourne for a decade.\n\nWhen I started at Fairfax, I was surprised to find the entire team running OSX. I wasn't surprised so much by the choice itself, as OSX is very popular in the Django and wider development community, but more so by my own lack of experience with it, having solely used Linux for the last half decade. I decided to give it a go and found it to be on par with Linux as a development environment.\n\nMy biggest gripe was breaking down the mental muscle I'd built up around the shortcut keys for wrangling text. At first I was fumbling on OSX, but after a few days of using OSX during the day and Linux at night, I found that I wasn't efficient with either of them - I needed consistency. I was also long overdue for a new machine. I originally had my eye on some of the MacBook Air clones like the Acer UltraBook and Asus ZenBook, but I couldn't find any information about running Linux on these, and I wasn't prepared to go through the pain of working it out if anything went wrong. So I bit the bullet and picked up a 13 inch MacBook Air.\n\nAll I need at the software level is visible application shortcuts, keyboard-driven application switching, a decent editor, terminal, web browser and package manager, and I'm good to go. In that regard, OSX and Linux are equivalent for my use, each with their own minor flaws. What I am really loving is the hardware. The keyboard seems laid out in a way that lets me type faster than ever, and the solid state drive means everything is instantaneous - that paired with the best battery life I've ever experienced, and it's a dream machine.", "published": 0, "publish_date": "2012-01-10T18:00:00", "slug": "2011-my-year-in-review", "title": "2011: My Year in Review"}}, {"pk": 9, "model": "blog.blogpost", "fields": {"content": "The two front-runners in source code management these days are undoubtedly Git and Mercurial. Distributed version control has clearly proven itself as the superior model over older centralised systems like SVN, particularly in the context of open source development, where the ability to fork repositories and push and pull branches between them facilitates a much more efficient and streamlined work-flow.\n\nMy personal opinion about the difference between the two is similar to my take on Django and Rails. Relative to their alternatives, the two are far more similar than different, essentially providing the same concepts and features, but with quite different underlying philosophies in their implementations. For this reason I choose the less popular Mercurial over Git to manage all of my projects. Firstly I feel it has a more natural and intuitive UI (I'm not referring to a graphical interface here, but the actual commands it implements and their arguments). Another key factor is that Mercurial is written in Python, which means hacking on it and building extensions for it is a breeze.\n\nWhat makes Git more popular than Mercurial? One major factor is their respective online hosting services, GitHub for Git and Bitbucket for Mercurial. If you're unfamiliar with these sites, they're like Facebook for programmers who share projects and collaborate together on code rather than post photos and videos. While both of these sites mostly implement the same core features, Github has always been several steps ahead of Bitbucket in terms of overall polish, and being first to market with new features. This difference has led GitHub to become far more popular than its Mercurial counterpart, and as a result, Git far more popular than Mercurial.\n\nWith the majority of open source activity occurring on GitHub, what then is a Mercurial user to do? Limiting your projects to the audience of Bitbucket means missing out on a lot of potential collaboration. Fortunately some time ago, the team at GitHub developed a Mercurial extension called hg-git. It allows you to transfer code back and forth between a Mercurial repository on your machine to a Git repository on another machine, like GitHub for example, taking care of all the translation required between Git and Mercurial. After several years of using hg-git, it's one of the only pieces of software that continues to amaze me. Think of Google's language translator, which on a good day can provide some very quirky translations when converting text from one language to another. hg-git performs the same task, but has no room for error when translating a source code repository from Mercurial to Git, and back again. Admittedly hg-git has a much easier job to do than translating human languages with all their ambiguities. Still, I am constantly impressed by the task it performs.\n\nSo hg-git allows me to develop my projects using Mercurial and have them shared on both Github and Bitbucket, allowing for maximum collaboration which is fantastic. It's not entirely seamless however. I still need to perform a couple of manual steps such as adding GitHub paths to my repo configuration, and creating Mercurial tags that map to the Git branches I want to work with. I need to do this each time I set up a new repository, be it for starting a new project, or forking a project of my own or someone else's. Recently I had to do this about half a dozen times in the space of an hour while working on a few different projects, and I thought to myself that I should be able to automate it. The result is a Mercurial extension I've called hg-github which automatically takes care of these manual steps required. It also wraps hg-git, so you don't need to install both extensions, as hg-github pulls in and takes care of all the hard work that hg-git does.\n\nOverview\n\nOnce hg-github is installed, assuming the default remote location of your repository is on Bitbucket, the GitHub path is automatically added and given the name github, so you can push to it with the following command:\n\n$ hg push github\n\n\n\nFor other named Bitbucket locations, the name github-NAME is given, where NAME is the name of the path located on BitBucket. For example consider the following .hg/hgrc repo config:\n\n[paths]\ndefault = ssh://hg@bitbucket.org/stephenmcd/hg-github\nsomefork = ssh://hg@bitbucket.org/stephenmcd/hg-github-temp\n\n\n\nhg-git will add entries to the config file as follows. Note that the config file isn't actually written to:\n\n[paths]\ndefault = ssh://hg@bitbucket.org/stephenmcd/hg-github\nsomefork = ssh://hg@bitbucket.org/stephenmcd/hg-github-temp\n\ngithub = git+ssh://git@github.com/stephenmcd/hg-github.git\ngithub-somefork = git+ssh://git@github.com/stephenmcd/hg-github-temp.git\n\n\n\nhg-github assumes you have the same username on GitHub and Bitbucket. If you have a different GitHub username, you can specify it by adding the following section to your global .hgrc file. For example my GitHub username is stephenmcd:\n\n[github]\nusername = stephenmcd\n\n", "published": 1, "publish_date": "2011-12-31T18:00:00", "slug": "announcing-hg-github", "title": "Announcing hg-github"}}, {"pk": 10, "model": "blog.blogpost", "fields": {"content": "As a creator and maintainer of several popular reusable Django applications, one of the most commonly requested features I'm asked for is the ability to customise the fields that a model implements. This topic comes up often on the Mezzanine mailing list, and during this particular thread we researched ways that fields could be dynamically injected into models at run-time.\n\nOther Approaches\n\nIt's worth taking a look at other approaches to the general problem, and what their drawbacks are, in order to provide context for what the final solution needs to achieve.\n\nOne approach is to implement as many of the model classes as possible as abstract base classes, so that users can subclass these with their own models. This approach makes sense for certain types of customisation, and it's what I've done with django-forms-builder for example. Some caveats exist with this approach however. Firstly, relationship fields can't be defined on the abstract models, so these need to be implemented in concrete models either within the same app, or by the user implementing their own subclasses. Secondly, any functionality that references your models, such as views or middleware, needs to either have configurable settings for choosing which models to use, or be reimplemented entirely by the user to make use of their custom fields.\n\nAnother approach is to simply recommend that users subclass the models that the app provides using multi-table inheritance. Unfortunately this will introduce unnecessary overhead with the extra database queries required when accessing the instances of the subclasses. Best case is that this amounts to an extra query or two in a view dealing with a single instance. Worst case is that when this approach is used with a queryset in a template, an extra query is performed for each instance returned - the classic N+1 query problem.\n\nDynamic Injection\n\nThe ideal approach would allow users to directly modify models with their own code, outside of the models' apps, without the models themselves having to implement any special hooks for customisation. The end result being an optimal database design, with no extra API requirements for the relevant models. It just so happens that this is possible by using several features that Django exposes, and combining them together in a particular way.\n\nThe approach boils down to three concepts:\n\n\n  Dynamically adding fields to model classes\n  Ensuring Django's model system respects the new fields\n  Getting the load ordering correct for the above to work\n\n\nDjango's model fields provide an undocumented contribute_to_class method. This method serves as a fancy version of setattr and takes a value and attribute name to use as arguments. Internally it then takes care of all the house-keeping required for a field to be added to a model.\n\nThe other feature of Django we'll use is the class_prepared signal. This signal is emitted each time a model class is declared and loaded for the first time by Django's model system.\n\nfrom django.db.models import CharField\nfrom django.db.models.signals import class_prepared\n\ndef add_field(sender, **kwargs):\n    \"\"\"\n    class_prepared signal handler that checks for the model named\n    MyModel as the sender, and adds a CharField\n    to it.\n    \"\"\"\n    if sender.__name__ == \"MyModel\":\n        field = CharField(\"New field\", max_length=100)\n        field.contribute_to_class(sender, \"new_field\")\n\nclass_prepared.connect(add_field)\n\n\n\nThe final consideration is connecting the class_prepared signal at the correct time. It needs to occur prior to the relevant model class being declared, otherwise the signal will never be triggered when we want it to. A general way of achieving this is to connect the signal from within an app that is listed before the app containing the relevant model, in the INSTALLED_APPS setting. Note that in the above code, we don't explicitly import the model to use it as the signal's sender, instead checking for the model's class name, as importing it would break these load ordering requirements.\n\nCaveats\n\nLike the previously described approaches, dynamic injection also comes with a set of drawbacks. These drawbacks stem from the fact that the apps containing the models being customised don't contain a definition for the fields being injected. This means that migration tools likes South are unable to detect the new fields, and workarounds such as creating manual migrations are required.\n\nAnother related problem is when new admin classes containing references to the custom fields are registered and the fields haven't yet been injected. A typical requirement for injected fields is to expose them via Django's admin interface, which can be achieved by unregistering existing admin classes for the models that fields are being injected into, subclassing these admin classes with new references to the injected fields, and registering the new admin classes. Unfortunately if this unregister/register dance occurs in an admin module, the fields may not have yet been injected. A quick work-around for this is to perform the unregister/register calls inside your project's urlconf.\n\nMezzanine Support\n\nDrawbacks aside, the field injection technique described above has characteristics that make it incredibly useful. As such the approach has first-class support in Mezzanine by way of the EXTRA_MODEL_FIELDS setting. This setting allows you to define a sequence of all the custom fields you'd like to inject. Each item in the sequence contains four items: the dotted Python path to the model (including the field name to use for injection), the dotted Python path to the field class to use for the injected field, a sequence of the field's position arguments, and finally a dict of its keyword arguments.\n\nEXTRA_MODEL_FIELDS = (\n    # Add a custom image field from the fictitious somelib.fields module\n    # to Mezzanine's BlogPost model:\n    (\n        # Dotted path to field.\n        \"mezzanine.blog.models.BlogPost.image\",\n        # Dotted path to field class.\n        \"somelib.fields.ImageField\",\n        # Positional args for field class.\n        (\"Image\",),\n        # Keyword args for field class.\n        {\"blank\": True, \"upload_to\": \"blog\"},\n    ),\n    # Example of adding a field to *all* of Mezzanine's content types:\n    (\n        \"mezzanine.pages.models.Page.another_field\",\n        \"IntegerField\", # 'django.db.models.' is implied if path is omitted.\n        (\"Another name\",),\n        {\"blank\": True, \"default\": 1},\n    ),\n)\n\n\n\nMezzanine then uses this setting to inject all of the fields defined, using class_prepared and contribute_to_class as described earlier. It handles getting load order correct by performing the injection within the mezzanine.boot app, which is forced to the front of all apps defined in INSTALLED_APPS. Django's admin is also patched in the boot app, to defer certain calls to unregister and register, to correct the ordering issues described earlier.", "published": 1, "publish_date": "2011-11-10T18:00:00", "slug": "django-model-field-injection", "title": "Django Model Field Injection"}}, {"pk": 11, "model": "blog.blogpost", "fields": {"content": "Update, Feb 14: @rkJun has translated this post into Korean.\n\nI recently gave a presentational talk to the development team at my work, on what it's like contributing to open source. A common perception by people not involved is that the open source community seems like one big socialist hippie commune, whose motivations are entirely altruistic with little reward for the individual. The approach I took was to dispel this myth and show that for many who participate, their motivations are mostly self serving, with many personal gains to be made. Despite some of the awkward truths I focused on, the talk was received really well.\n\nYour Day Job Sucks, Make Programming Fun Again\n\nIf you have a regular 9 to 5 job as a software engineer at a typical company, chances are likely that your job sucks in several ways. You work on the same types of projects every day. You work with the same technology every day, and most likely it's dated. There are more modern technology stacks out there that are much more elegant, yet for a variety of non-technical reasons, you're not allowed to use them at work. You also have deadlines. Deadlines lead to knocking out features as quickly as possible, which directly conflicts with the quality of your work, so you're often forced to take ugly shortcuts, leaving little to be proud of.\n\nIf this is your only exposure to programming, it gets boring pretty quickly and isn't much fun at all. Contrary to this, programming can actually be incredibly fun! Imagine your company had hundreds of different projects to choose from, and let you choose whichever one you wanted to work on. Then on top of that, they told you that you could use whichever technologies you wanted to, and that you could take as long as you needed to build it correctly - so long as the end result was the most well designed and beautifully written software you've ever produced. As a professional programmer, that sounds like a dream, but that's exactly how it is when you work on open source projects. Choose your problem domain. Choose your technology stack. Choose your pace. Choose your quality.\n\nBecome a Better Developer through Collaboration\n\nThere are two ways to become a better developer: writing code and reading code. This clearly doesn't make sense in a silo on your own. There's little to learn from reading your own code beyond reminiscent value, and you could write code on your own for years and the only lessons learned would be those you teach yourself. Naturally the real learning is to be had from interacting with other developers, writing code that they can review and give you feedback on, and reading code written by other developers with more experience and different approaches than you have.\n\nDrawing back to your day job, how many developers do you collaborate directly with. A few? A dozen? A small pool that will give you much to learn from, but is still relatively limited in scope. The open source community is made up of thousands of developers, who over many years have produced millions of lines of code. Code of quality much greater than you'd ever be exposed to without stepping outside of your workplace.\n\nAcquire Cash Money\n\nOne of the biggest misconceptions abut open source development is that it's a financially fruitless labour. While true to a certain extent, there are actually several ways in which it can pay a monetary return, both indirectly and directly.\n\nYou may work a steady, full-time job, or you're looking for one, and think of yourself as an employee with a fixed salary, in total contrast to those wild freelancers and entrepreneurs who don't know where their next meal will come from. Wrong. You are a business, just like your employer is. You have one customer, your employer. Sometimes you might feel as though you're stuck in your day job, when in actual fact you're simply stuck with one client.\n\nSo how can you gain more clients? How can you gain a better client? By promoting your business of course. Open source provides a great way for you to showcase you and your ability as a developer. Your open source contributions can distinguish your CV from other developers, namely, your business's competition. Graphic designers have always had the luxury of being able to display their portfolios, and developers can too. I've been on both sides of the hiring table, and when choosing between potential hires, someone with significant open source contributions has already demonstrated a clear passion for what they do, and therefore has a major advantage over those who are tasked with convincing people of the same, without having anything to show for it.\n\nGetting your open source projects to the level of quality and usefulness where people are actually building production systems with them is huge achievement in itself. Stemming from that are direct opportunities for paid work. Your corporate users may need your software customised for their use cases. You may even be lucky enough that they'd like features developed into the project that are great ideas - things you might not have thought of, and will take the project to the next level. Who better to develop these than the creator of the software themselves? I've had several instances where this has occurred, and companies have sponsored development on features that, had I had the insight to think of, would have developed freely on my own anyway.\n\nBroaden Non-Technical Experience\n\nHands on coding is only one small aspect of software development. There are many other disciplines that make up the process of shipping a successful product. Project management, community building, formal writing, product development and marketing. While freelancers and one-man commercial studios may get the benefit of dipping their toes into these different areas, as a developer at a typical company your role often doesn't take you outside of the day to day coding.\n\nRunning an open source project properly will allow you to skill up in broad range of non-technical areas. I spend a lot of my time working on my most popular project, however these days only a small portion of this time is actually spent coding. Most of the time is spent in discussion with other developers on their feature ideas, coordinating the inclusion or exclusion of these, and providing support for new users. It's hard work, but it's a great and relevant experience.\n\nCorollary to this, many non-technical people who'd like to contribute to open source in some way, believe that they don't have anything worthwhile to offer. With all that's involved, some of the most beneficial contributions are those of a non-technical nature. If this describes you, then please don't hesitate to dive in. Your help means more than you can imagine.\n\nFeels Good Man\n\nThe most obvious yet least tangible benefit in contributing to open source, is the idea I first alluded to around altruism and giving to others. As I mentioned, coordinating a non-trivial open source project can be hard work. It has its highs and lows, and sometimes it feels like there's more of the latter. However every now and then, I'll get a personal email of praise and gratitude. Not in any public forum where the soapbox can be a conduit for ulterior motives, just a private and genuine expression of thanks. I would never have thought of this as being particularly rewarding, until I actually experienced it. It really fills you with a true sense of pride and satisfaction, which makes it all worth while in the end.\n\nGet Started Now\n\nThe Chinese philosopher Lao Tzu said:\n\n\n  A journey of a thousand miles begins with a single step\n\n\nGetting started might seem like a daunting task. Start small. Ignore the inner voice that whispers in your ear, arguing that your open source idea wouldn't be of any use to anyone. That might even be true, but it doesn't matter. Do it anyway. A lot of my projects are only a few dozen lines of code. People have found them useful in ways I couldn't have imagined.\n\nIdeas can be hard to come by. Most of mine were itches I wanted to scratch. Product gaps in the technology stacks I'm interested in, or mundane processes I went through regularly, upon realising they could be automated and productised.\n\nPerhaps you're an end user of open source software, and it isn't perfect. A feature works differently than how you think it should, or something is undocumented and you struggled to find it. These days it seems that the modus operandi is to throw up your arms, and complain as loudly as possible about how the software you're using for free, isn't exactly the way you want it to be. Don't. You have the choice to do something about it. Dive into the source code. Contribute something back. Improve yourself, and be a part of something.", "published": 1, "publish_date": "2011-09-12T17:00:00", "slug": "open-source-for-you", "title": "Open Source for You"}}, {"pk": 12, "model": "blog.blogpost", "fields": {"content": "I recently took part in Django Dash, the annual\nhackathon where teams of up to three compete to build the best\nDjango project they can within 48 hours. This year\nI worked with Travis White and Josh de\nBlank to build DrawnBy -\na collaborative drawing application that allows people to sketch together in\nreal-time.\n\nDrawnBy makes extensive use of\nWebSockets which are mostly unheard\nof in web stacks like Django and Rails, or even antiquated stacks like PHP and\nASP.NET, which are all designed around accepting a request and returning a\nresponse to the browser as quickly as possible. This is in contrast to\nWebSockets which allow full duplex communication between the browser and\nserver, and therefore require long running requests per user.\n\nA variety of patterns for dealing with WebSockets in Django emerged while\ndeveloping DrawnBy, and since the Dash I've been working on abstracting these\ninto a reusable Django application called django-socketio which I've released\ntoday. It's available on Github, Bitbucket and\nPyPI.\n\nHere's an overview of the features.\n\nFeatures\n\n\n  Installation of required packages from PyPI\n  A management command for running gevent's pywsgi server with auto-reloading capabilities\n  A channel subscription and broadcast system that extends Socket.IO allowing WebSockets and events to be partitioned into separate concerns\n  A signals-like event system that abstracts away the various stages of a Socket.IO request\n  The required views, urlpatterns, templatetags and tests for all the above\n\n\nChannels\n\nThe WebSocket implemented by gevent-websocket provides two methods for sending\ndata to other clients, socket.send which sends data to the given socket\ninstance, and socket.broadcast which sends data to all socket instances\nother than itself.\n\nA common requirement for WebSocket based applications is to divide\ncommunications up into separate channels. For example a chat site may have\nmultiple chat rooms and rather than using broadcast which would send a chat\nmessage to all chat rooms, each room would need a reference to each of the\nconnected sockets so that send can be called on each socket when a new\nmessage arrives for that room.\n\ndjango-socketio extends Socket.IO both on the client and server to provide\nchannels that can be subscribed and broadcast to.\n\nTo subscribe to a channel client-side in JavaScript use the socket.subscribe\nmethod:\n\nvar socket = new io.Socket();\nsocket.connect();\nsocket.on(connect, function() {\n    socket.subscribe(my channel);\n});\n\n\n\nOnce the socket is subscribed to a channel, you can then broadcast to the\nchannel server-side in Python using the socket.broadcast_channel method:\n\nsocket.broadcast_channel(\"my message\")\n\n\n\nEvents\n\nThe django_socketio.events module provides a handful of events that can be\nsubscribed to, very much like connecting receiver functions to Django signals.\nEach of these events are raised throughout the relevant stages of a Socket.IO\nrequest.\n\nEvents are subscribed to by applying each event as a decorator to your event\nhandler functions:\n\nfrom django_socketio.events import on_message\n\n@on_message\ndef my_message_handler(request, socket, context, message):\n    ...\n\n\n\nEach event handler takes at least three arguments: the current Django\nrequest, the Socket.IO socket the event occurred for, and a context,\nwhich is simply a dictionary that can be used to persist variables across all\nevents throughout the life-cycle of a single WebSocket connection.\n\n\n  on_connect - occurs once when the WebSocket connection is first established.\n  on_message - occurs every time data is sent to the WebSocket. Takes an extra message argument which contains the data sent.\n  on_subscribe - occurs when a channel is subscribed to. Takes an extra channel argument which contains the channel subscribed to.\n  on_unsubscribe - occurs when a channel is unsubscribed from. Takes an extra channel argument which contains the channel unsubscribed from.\n  on_error - occurs when an error is raised. Takes an extra exception argument which contains the exception for the error.\n  on_disconnect - occurs once when the WebSocket disconnects.\n  on_finish - occurs once when the Socket.IO request is finished.\n\n\nLike Django signals, event handlers can be defined anywhere so long as they\nend up being imported. Consider adding them to their own module that gets\nimported by your urlconf, or even adding them to your views module since\nthey're conceptually similar to views.\n\nBinding Events to Channels\n\nAll events other than the on_connect event can also be bound to particular\nchannels by passing a channel argument to the event decorator. The channel\nargument can contain a regular expression pattern used to match again multiple\nchannels of similar function.\n\nFor example, suppose you implemented a chat site with multiple rooms.\nWebSockets would be the basis for users communicating within each chat room,\nhowever you may want to use them elsewhere throughout the site for different\npurposes, perhaps for a real-time admin dashboard. In this case there would be\ntwo distinct WebSocket uses, with the chat rooms each requiring their own\nindividual channels.\n\nSuppose each chat room user subscribes to a channel client-side using the\nroom's ID:\n\nvar socket = new io.Socket();\nvar roomID = 42;\nsocket.connect();\nsocket.on(connect, function() {\n    socket.subscribe(room- + roomID);\n});\n\n\n\nThen server-side the different message handlers are bound to each type of\nchannel:\n\n@on_message(channel=\"dashboard\")\ndef my_dashboard_handler(request, socket, context, message):\n    ...\n\n@on_message(channel=\"^room-\")\ndef my_chat_handler(request, socket, context, message):\n    ...\n\n\n\nChat Demo\n\nThe 'hello world' of WebSocket applications is naturally the chat room. As\nsuch django-socketio comes with a demo chat application that provides examples\nof the different events and channel features available.", "published": 1, "publish_date": "2011-08-13T17:00:00", "slug": "real-time-web-apps-with-django-and-websockets", "title": "Real-time Web Apps with Django and WebSockets"}}, {"pk": 13, "model": "blog.blogpost", "fields": {"content": "I recently started a new role at a Ruby on Rails shop, which as a long time Django specialist was a really interesting opportunity. There's a lot of competition between the two frameworks' communities, ranging from friendly rivalry and respectful admiration at the mature end of the scale, to all out fanboy fuelled flame-wars at the other.\n\nAfter you wade through the rivalry, the common wisdom voiced is that they're conceptually the same. If you know Python, go with Django and if you know Ruby, go with Rails. After spending several months with Rails I can attest to this being true. At a bird's-eye view both frameworks contain almost identical concepts, implemented with different philosophies stemming from the ideals expressed by the languages they're written in. Both frameworks provide a vastly superior approach to security, modularity and rapid development than their predecessors do.\n\nAn interesting question to ask would be: which would be the best framework to choose, not knowing either language? It would be naive of me to believe I am unbiased, but I would certainly recommend Django over Rails. The relative strictness of Python and explicitness of each component in Django, compared to the implicit magic in Rails, is simply much more geared towards creating large-scale systems in a sane and transparent way. To Ruby's credit though, I have developed a real admiration for the language itself, and have continued using it in my own projects - but that's a topic for another post.\n\nConsidering how similar the two frameworks are component-wise, one thing I did miss was a side-by-side cheat sheet for working out what each of the concepts were in Rails that I already knew well in Django. I've put one together below to help out anyone who might be picking up either framework while already knowing the other. For clarity, I've also included descriptions of each type of component, for those who haven't used either framework.\n\n\n\n    Django\n    Ruby on Rails\n    &nbsp;\n\n\n    URL patterns\n    Routes\n    Regular expression definitions for each type of URL and what part of the web site they map to.\n\n\n    Views\n    Controllers\n    The units of code that the above regular expressions map to, that perform application logic and pass data to a rendering layer.\n\n\n    Templates\n    Views\n    The rendering layer that is given data from the code described above, and performs display logic typically wrapped around HTML code.\n\n\n    Template tags (built-in)\n    Embedded Ruby\n    The flow-control language that can be used in the rendering layer.\n\n\n    Template tags (custom)\n    Helpers\n    The system for defining custom functions that can be used in the rendering layer.\n\n\n    Models\n    Models\n    The data-definition layer that maps classes to database tables - the ORM.\n\n\n    Managers\n    Scopes\n    The way to extend the ORM to define custom database queries.\n\n\n    Management commands\n    Rake tasks\n    Scripts for performing administrative tasks via the command line.\n\n\n    Project\n    App\n    An entire application built with the framework.\n\n\n    App\n    Plugin\n    The way in which all components in the framework can grouped together in separate areas of functionality.\n\n\n    South (third-party)\n    Migrations\n    The system used for automatically applying changes in the ORM definition to the underlying database tables, such as adding and removing columns.\n\n\n    Admin\n    RailsAdmin (third-party)\n    A web-based interface for authenticating administrative users and providing CRUD tools for managing data.\n\n\n    \n    The following table lists software that aren't part of Django or Rails, but are core parts of the Python and Ruby eco-systems, and go hand-in-hand with using either framework.\n    \n\n\n    Python\n    Ruby\n    &nbsp;\n\n\n    Virtualenv\n    RVM\n    The system used for running isolated environments bound to a particular language version, combined with a set of install libraries.\n\n\n    PIP\n    Bundler\n    The package manager for installing libraries.\n\n\n    WSGI\n    Rack\n    A standard specification for applications to interface with HTTP, allowing for a single application entry point and middleware to be implemented.\n\n\n    Fabric\n    Capistrano\n    A system for automating tasks on remote servers from a local machine, typically as part of a deployment process.\n\n\n\nNot all of these pairings are a perfect one-to-one match conceptually, but should be good enough to get an overall view of what each concept is within both frameworks.", "published": 0, "publish_date": "2011-07-30T17:00:00", "slug": "rails-quick-start-for-djangonauts", "title": "Rails Quick Start for Djangonauts"}}, {"pk": 14, "model": "blog.blogpost", "fields": {"content": "The question of scalability with regard to\nMezzanine and\nDjango recently came on up on the\nmezzanine-users mailing list, to which I offered the following reply.\n\nMezzanine and Django itself are fantastic choices for someone concerned with\nscaling for high traffic.\n\nThe Mozilla add-ons site that hosts all Firefox\nplugins, and Disqus which is currently the world's\nhighest traffic commenting system, both run on Django. Each of these have been\nquoted at 500 million hits per day and 1\nbillion per month respectively.\n\nOne of the keys to scaling sites like these is the wealth of options available\nfor caching in the Django ecosystem, and the ability to then scale out the\nnumber of both application servers running Django, and caching servers\ntypically running memcached, with very little\nmodification to your application code. Django comes out-of-the-box with the\nability to switch on site-wide\ncaching, per page caching and template fragment\ncaching. Beyond that there are also third-party Django applications\nthat implement object level caching such as Django Cache\nMachine and Johnny\nCache, for even finer-grained\ncontrol with little modification to your code.\n\nBoth Mezzanine and Cartridge have been designed\nfrom the ground up with scalability in mind. Particular care has been taken to\navoid any n+1 queries, for example rendering out multiple\ninstances of Mezzanine's navigation tree containing any number of nested\nlevels of navigation will only ever run a single database query. Same with\nCartridge's products. What this means is that you can go very far traffic-\nwise, using a single server without even thinking about caching. Once you do\nthen the ability to add application and cache servers is trivial, and will\ntake you incredibly far using a single database. Once you start needing\nmultiple database servers Django also comes with the built-in ability to\nroute models across different\ndatabases, so\nthere are extra options there beyond your typical master/slave database\nreplication scenario.\n\nI've only touched on the topic to provide an overview of what's available, and\nyou should certainly research it further for your particular scenario, but as\nyou can see scalability is a core concern baked into Django and Mezzanine, so\nyou can choose these with confidence.", "published": 1, "publish_date": "2011-06-16T17:00:00", "slug": "does-mezzanine-scale", "title": "Does Mezzanine Scale?"}}, {"pk": 15, "model": "blog.blogpost", "fields": {"content": "Mezzanine and Cartridge Hit the Mainstream\n\nThis week I had the fantastic opportunity to be interviewed by Rodney\nGedda from IDG for their\nTechWorld and CIO\nmagazines. The interview focused on my open source work with the\nMezzanine and\nCartridge projects.\n\nMelbourne hacker releases open source Django apps\n\nThese projects have been gaining traction at an incredible pace, with a solid\ncommunity forming around them over the last year. All of this has happened in\nspite of having no formal promotion outside of the community itself, which\nmakes this type of exposure all the more exciting.\n\nIt's also no secret that I think the Australian web development market is\nmany years behind the United States, so it's a great step forward to see modern\ntechnology getting covered by the mainstream local tech media.", "published": 1, "publish_date": "2011-03-21T17:00:00", "slug": "mezzanine-and-cartridge-hit-the-mainstream", "title": "Mezzanine and Cartridge Hit the Mainstream"}}, {"pk": 16, "model": "blog.blogpost", "fields": {"content": "Having remained utterly faithful to Python and\nDjango over the last year, and dedicating all\nof my available time to open source projects like\nMezzanine and\nCartridge, with the new year at hand I though it\nwas about time to take a break and add some new technology to my repertoire.\nDuring his keynote speech at Djangocon.eu\n2010, the creator of Django Jacob\nKaplan-Moss states \u201cit will challenge what you think\nyou know about web/server architecture\u201d when referring to\nNode.js. Since then it has been sitting in the back of my\nmind as something I definitely needed to check out, so I decided to dive in\nhead first.\n\nNode.js is a general purpose JavaScript development environment geared towards\nwriting network servers. It uses an event-based, non-blocking architecture\nwhich allows your web application to scale to thousands of concurrent\nconnections without needing a finite pool of threads or processes. JavaScript\nis executed using Google's V8\nengine which ranks very highly in\nspeed compared to other dynamic\nlanguages, so not only does Node.js scale elegantly, it's damn fast.\n\nHaving recently built Grillo, a console\nbased chat server, I'd been considering what it would be like to put together\na web-based version. In fact I had achieved something\nsimilar in the past using Python's\nBaseHttpServer module,\nand while functional for a few hundred connections, my approach would never\nscale, as either a separate thread or process would be required for each open\nconnection. The event driven architecture of scaling a web server for an\nincreasing number of open connections is mostly a solved problem, especially\nin the Python community with projects like\nTwisted and\nTornado. However Node.js is different in that\nits non-blocking evented model is a first class citizen by design.\n\nWith a useful project at hand to try Node.js on, I set about creating what\nI've named Grillode (yes you guessed it: Grillo\n+ Node). It's a web-based chat server with a set of configuration options that\nlets you run it in various modes, such as a customer support queue, or with\nChatroulette style random match-\nups. I've released the source onto\nGithub and\nBitbucket, and also have a demo\nup and running.\n\nThe process of putting Grillode together led me through many parts of the\necosystem that has developed around Node.js - following is an overview of the\npieces I ended up working with.\n\nNode Package Manager (NPM)\n\nNPM is a command line utility that gives you access to a\ncentral online repository of packages built for Node.js. It works wonderfully\nwhen installed correctly, but on my machine I encountered a handful of issues\nwhere it ended up recreating various system directories all throughout my home\ndirectory. After setting up various symlinks by hand, I did get it to work\nafter many failed attempts at installing it. This issue was definitely\nspecific to my machine as I was then able to go ahead and install NPM\nseamlessly on several different servers.\n\nOnce everything was working correctly it made deployment of Grillode a breeze.\nBy specifying all of its dependencies in a package.json file, NPM was able to installed\neverything required in a single step.\n\nExpress\n\nExpress provides basic URL routing to functions that\nwill typically perform some application logic and hand off data to a template\nto be rendered. It contains integration points for a handful of different\ntemplating libraries and it also contains a simple middleware system. I\u2019d\ndefinitely consider it to be a micro framework, but it\u2019s a great start at\nwhipping your Node.js application into a well defined structure.\n\nSocket.io\n\nSocket.io takes all the leg work out of maintaining an\nopen connection to the browser. You start by attaching it to your Node.js\nserver which then automatically makes available the client-side JavaScript\nrequired. This provides the communication channel between the client and the\nserver, which attempts to use web\nsockets when available, and\ntransparently falls back to Flash sockets or even old-school AJAX polling if\nthe former options aren\u2019t supported by the browser.\n\nIt then provides all of the methods and event handlers for connecting and\nsending data over the connection. The beauty behind how this is implemented is\nthat it exposes these methods and events almost identically to both the\nNode.js server, and the browser client - instantly you have available two-way\ncommunication between the browser and the server via an open connection,\nwithout requiring any new requests to the server.\n\nCoffeeScript\n\nAs many others have done, I\u2019ve often compared JavaScript to Python in that\nthey both share an object model defined by a hash table of names and object\nmembers, which can be introspected and dynamically modified. While this is a\nvery elegant model, JavaScript boasts syntax reminiscant of the turn of the\ncentury, cluttered with semicolons and braces, and missing a handful of\nfeatures found in modern languages such as list comprehensions and much more.\nWell the war on semicolons is over with the explosion in\npopularity of languages such as Python and Ruby showing this to be true.\n\nCoffeeScript is a language inspired by Python and\nRuby which gets compiled directly into JavaScript. It therefore retains all\nthe properties of JavaScript such as its data types, objects and methods, but\nprovides a much more modern and clean syntax with some fantastic sugar, such\nas list comprehensions, class-based object construction, string interpolation\nand more.\n\nExperimentally, CoffeeScript can be run directly in the browser in place of\nJavaScript by including the compiler JavaScript file itself, but much more\ninterestingly it can be used as an execution environment for Node.js, which\nwill perform the compilation to JavaScript when the Node.js application is\nfirst started. I found my experience to match reports of up to 30% in\nreduction of the amount of code required.\n\nCoffeekup\n\nWhen looking into what was available for templating, I started out with\nEJS which basically gives you executable\nJavaScript within your template files. I found this to provide a poor\nseparation between display and application code, something that Django gets\nright by providing a limited template language that has basic flow control,\nbut makes writing non-trivial logic difficult to do.\n\nI then discovered a Node.js template library called\nCoffeekup, that tied in very closely with the time I\nhad already spent with CoffeeScript. Coffeekup allows you to define your HTML\nentirely in CoffeeScript. I\u2019m still undecided on whether this is a thing of\nbeauty or horror. It\u2019s very surreal to work with web page markup expressed\nentirely in programming code. I guess there\u2019s somewhat of an undeserved\nfeeling of the presentation being too closely tied to programming logic. There\nis a magical feeling in having your server code, client code and presentation\ncode all in the exact same language, coupled with the given language being\nCoffeeScript which is incredibly clean.\n\nConclusion\n\nI really enjoyed working with Node.js and the young ecosystem surrounding it.\nAt this point in time, I wouldn\u2019t consider it for a typical project over a\nfull stack framework like Django with the elegance of Python, however it\ndefinitely serves as a fantastic choice for a very specific criteria -\nscalable, real time web applications.", "published": 0, "publish_date": "2011-02-01T18:00:00", "slug": "how-i-now-know-node", "title": "How I Now Know Node"}}, {"pk": 17, "model": "blog.blogpost", "fields": {"content": "I recently came across the Rosetta Code Project.\nIt's a community contributed wiki that contains hundreds of solutions to\nprogramming problems,\nimplemented in hundreds of different programming\nlanguages, which\nis great source of entertainment for a programming languages enthusiast such\nas myself. The main focus of the project isn't to demonstrate individual\nsolutions on their own, but to provide comparisons between different\nprogramming languages and how they approach the same task.\n\nThe wiki also contains various dynamic reports, such as which tasks have yet\nto be implemented for each particular programming language. I took a look at the\nPython\npage to\nsee if there were any interesting tasks remaining that I could potentially\nprovide a solution to, and as I expected there were only a few relatively\nobscure tasks that were yet to have solutions provided.\n\nOne task remaining for Python that did catch my eye was to demonstrate a\nsimple chat server using sockets.\nI've always been especially fond of network programming, from web crawlers to\nXMLRPC to lower level sockets, I seem to really enjoy writing code that runs\nover the Internet without necessarily being related to web development, so I\nwent ahead and added a Python solution for the chat server\ntask.\n\nI honestly had so much fun working on this that I decided to extend it even\nfurther. Firstly I refactored it into a more object oriented approach which\nallowed for creating both server and client tools which could be run\nconcurrently using separate threads of control. I then added several commands\nthat could be run directly in chat by any user, for example listing the\ncurrent users who were logged in.\n\nI then decided to publish the code for my chat server and client onto both\nGitHub and\nBitbucket, after giving it the name\n'Grillo'. It's named after the Italian\nphone of the same name,\ndeveloped in 1965. They both share the common theme of being a very small\ncommunications device for their class, while being implemented with relatively\nbasic technology.\n\nGiven that there are many richer and more powerful applications available for\nimplementing the features Grillo provides, at the least it serves as a good\nexample of how to do basic socket programming in Python, as well as a\ndemonstrating some simple tricks for controlling threads. At the best case\nsomeone will pick up the code base and extend it further in ways I haven't\nanticipated - here's hoping!", "published": 1, "publish_date": "2011-01-03T18:00:00", "slug": "grillo-a-terminal-based-chat-server-and-client", "title": "Grillo, a Terminal Based Chat Server and Client"}}, {"pk": 18, "model": "blog.blogpost", "fields": {"content": "2010 was an amazing year for me professionally. I learnt and achieved much\nmore than I've historically packed into a single year, so like many other I've\ndecided to jump on the year in review bandwagon and put this post\ntogether.\n\nOpen Source\n\nPrior to 2010 I'd certainly used open source quite heavily, in fact I'd based\nmy entire career as a developer around it, focusing on\nPython, Django and\nLinux and the open source ecosystem\nsurrounding this amazing platform. However as far as giving back to the\ncommunity went, I had only ever contributed small bug fixes and enhancements\nto a handful of open source projects over the years, and had never been\nheavily involved in any single project. So in 2010 I dived in head first\nlaunching a range of open source projects. Firstly some smaller utilities such\nas django-forms-builder\nand gunicorn-console, and\nthen the Django content management platform\nMezzanine, along with its shopping cart plug-in,\nCartridge.\n\nThe reception Mezzanine has received has been nothing short of amazing and\nwell beyond anything I had anticipated. It's been an incredibly rewarding\nlearning experience, managing product development and working closely with its\ncontributors towards growing its community. Here are a few stats for the\nproject only 6 months since it was launched:\n\n\n  120 followers and 30 forks across GitHub and Bitbucket\n  Code contributions from 10 developers\n  A mailing list with over 60 members and over 300 messages\n  Over 4,000 downloads from the Python Package Index\n  Almost 9,000 visitors to the website with over a 50% return rate\n\n\nTeam Leadership\n\nShortly prior to 2010 I moved into the role of development team lead and 2010\nprovided me with a wealth of new experience in this regard. Being responsible\nfor a development team producing rock solid work has always been an aspiration\nof mine and it was incredibly fulfilling to have that come to fruition. It's\nbeen a fairly painless experience due to working with some of the best\ndevelopers I've met in over a decade, and it's been nothing short of amazing.\n\nTooling\n\nI added a ton of new software tools to my arsenal, being largely responsible\nfor building out our development and deployment processes. I spent a lot of\ntime getting up to speed with some amazing software such as\nSouth, Fabric,\nNGINX and gunicorn. I've\nrecently been putting together a development and deployment guide that covers\ncombining all of these and other parts of our stack which I'll also publish in\nthe near future, so stay tuned for that.\n\nDjango Dash\n\nThis was the first year I decided to put my Django skills to the test by\nentering Django Dash - the 48 hour hackathon where\nteams of 3 build a Django project from scratch. Teaming up with Josh de\nBlank and Andrew Fisher,\nthe final result saw us launch Rate My Flight which\nearned us 8th place out of 40 teams internationally. It was a ton of fun\ngetting to interact with some of the wider Django community and I even picked\nup a few new Django tricks along the way.\n\nSo that was 2010 - what a blast! I can't imagine what 2011 will bring if it\ninvolves as much change and initiative as the previous year has. Bring it on!", "published": 1, "publish_date": "2011-01-01T18:00:00", "slug": "2010-my-year-in-review", "title": "2010: My Year in Review"}}, {"pk": 19, "model": "blog.blogpost", "fields": {"content": "Web development technology has come an incredibly long way over the last\ndecade. Unfortunately in local markets like Australia where I reside, the tech\nsector seems to languish years behind the United States and Europe. We saw\nthis happen with broadband adoption at the turn of the century where most\nAustralians were still on dial-up for years after the rest of the world\nenjoyed widely available cable and ADSL.\n\nFast forward to 2010; and the same situation has occurred with web\ndevelopment. Anyone who follows the startup scene focused around San Francisco\nand NYC will be familiar with the most popular development technologies\navailable - Ruby,\nPython, Scala and many\nmore. Contrast this to the Australian space where the majority of development\nhouses have been sitting for many years on technologies such as ASP.NET and\nPHP, which reached their peak in popularity over half a decade ago.\n\nWhy are the majority of shops in markets like mine so complacent when it comes\nto their technology stacks? Is it CTOs that haven't written a line of code in\nyears and their fear of the unknown? Is it a lack of willingness to invest in\nkeeping their developers' skill-sets up to date and marketable? On these\nthings I can only speculate. What I can speak about knowledgeably however are\nsome of the reasons these latest technologies far outshine their predecessors,\nand if only a single technology manager reads this post and decides to act on\nit then it was well worth the time spent writing it.\n\nWe chose Django several years ago so that's\nwhat I'll be making reference to in the following comparisons, however you\ncould just as easily swap it out with Ruby on Rails\nor any other modern platform and the points would be more or less equivalent.\n\n\nASP.NETPHPJavaDjangoEfficiency\u2718\u2718\u2718\u2714Security\u2718\u2718\u2714\u2714Freedom\u2718\u2714\u2718\u2714Developers\u2714\u2714\u2714\u2718Mature Applications\u2714\u2714\u2714\u2718\n\nEfficiency\n\nI recently came across this fantastic quote:\n\n\n  'IDEs: a form of automation needed when the environment in question erects\nartificial barriers.'\n\n\nHave you ever tried writing C# or Java in a plain text editor? It is an\nexercise in futility as these languages sport incredibly verbose syntax and\ndeeply nested libraries which require specialised tools simply to write code.\nWhat about the developer that needs to work with all of these different\ntechnologies each day, can they be expected to be experts in several different\nIDEs and switch between them freely? If only these different programming\nenvironments could be used from the same editor - what a joy it would be as a\npracticing polyglot!\n\nOn the plus side languages like C# and Java are relatively clean and\nconsistant when compared to abominations such as PHP, which truly is a\ndisorganized mess - functions named using both verb_noun and noun_verb, lots\nof similar functions with no apparent naming convention (Eg: sort(),\narsort(), asort(), ksort(), natsort(), natcasesort(), rsort(),\nusort(), array_multisort(), uksort()) and a weak type system that can\nlead to bugs which are difficult to discover.\n\nThese languages stand in great contrast to modern dynamic languages such as\nPython and Ruby. Ask any Python or Ruby developer which IDE they use and the\nmajority of them will tell you they don't need one. These languages are terse,\nuse flat heirarchies in their libraries and are incredibly expressive.\n\nSecurity\n\nAside from the recent Padding Oracle\nExploit, ASP.NET has\nremained fairly secure over the years. Unfortunately the other parts of the\nstack, IIS and SQL Server, that it's exclusively tied to have been the\npunching bag of the network security world throughout the last decade with\nviruses such as Code\nRed, SQL\nSlammer and more, leaving countless\nwebsites either defaced or knocked entirely offline. With a track record like\nthis it truly is a wonder how anyone would knowingly choose to build public\nfacing Internet services based on a Windows stack.\n\nInversely we have PHP that while typically deployed on a LAMP stack built with\nsecurity in mind, the language itself makes writing secure applications an\nextemely disciplined task. One need look no further than the ongoing range of\nsecurity issues\nthat have plagued applications such as Wordpress over the years: SQL\ninjection, cross site scripting (XSS), remote code execution - it's like an\nall-you-can-eat smorgasbord of web application exploits.\n\nDjango in contrast runs on top of a secure LAMP stack and is designed from the\nground up with security in mind. It's protected by default against SQL\ninjection, XSS and cross site request forgeries. A developer would actually\nhave to make a concerted effort to create an exploitable Django application.\nAlso like many open source projects a security issue in Django isn't dealt\nwith because a corporation deems it to be the most cost effective decision. In\nthe very few and far between occasions when security issues have inevitably\nbeen discovered, turn-around time for resolving these has been within a 24\nhour period - not weeks or even months as is often the case with corporate\nvendors that lack the agility and motivation to act responsibly.\n\nFreedom\n\nA common misconception about open source software is that it lacks the\nreliability of support that comes with choosing a commercial vendor. This is a\nshort sighted view now plaguing many businesses. When Microsoft introduced\n.NET it made the skill-sets of thousands of VB developers redundant. What\nhappens when Microsoft announces that .NET is to be deprecated in their next\ntechnical adventure? The problem here is that a public company with an\nobligation to generate as much profit as it can controls the technology path\nof billions of dollars of software. Sometimes it's in their best financial\ninterest to create fantastic technology, and sometimes it's in their best\nfinancial interest to tear it all down again.\n\nThere is then the issue of acquisition. Companies like Microsoft and Oracle\nhave a long and successful history of acquiring their competitors simply to\ndiscontinue their competing technology - let's hope the vendor you're in bed\nwith isn't too good.\n\nEven Java, which for all intents and purposes is an open source technology has\nrecently shown that it isn't immune to the flaw of corporate ownership with\nOracle suing Google\nover its use on Android phones. Will Android developers find that their time\nand effort invested in this platform will all be for naught?\n\nPython and Django are both licensed under Permissive Free Software\nLicenses,\nwhich allow anyone to go ahead and do whatever the hell they like with them.\nThey are owned by the Python Software Foundation\nand the Django Software Foundation\nrespectively. These are non-profit bodies that for the most part exist to\nenforce the IP rights of each technology. These technologies are community\ndriven with one goal in mind: to create best of breed technology. There is no\nfinancial motivation here and so we thus find ultimate reliability with this\nsoftware stack being impervious to the risks described above - it cannot be\nmade redundant by any financially driven corporate strategy as the licensing\nand foundations have been specifically put in place to prevent this from being\npossible. An interesting corollary to this is that these technologies go\nrelatively unheard of without the backing of large corporations promoting\nthem. Next time you're the target of a technology sales pitch consider the\nhigh likelihood that you're not looking at the best technology in its given\napplication domain - best of breed doesn't need to be sold at all, it sells\nitself.\n\nDevelopers\n\nASP.NET, Java and PHP developers outnumber Python and Ruby developers by the\nhundreds if not thousands. This is a great selling point to technical decision\nmakers - the ability to quickly and easily hire developers when the need\narises is critical. But what of the quality of these developers? An\ninteresting question to pose is why a developer who specializes in .NET or PHP\nchose their particular technology. Answer: because that's what everyone else\nuses. You certainly won't find a developer who has gone out and thoroughly\ninvestigated a broad range of different languages coming back and choosing\n.NET over everything else. Those who have done so have chosen their languages\non its technical merits. These are the passionate developers with a love for\ntheir craft, not those who are merely in it for a paycheck and whose\nworkmanship will reflect as much. Paul Graham referred to this in 2004 in his\nincredibly insightful essay The Python\nParadox.\n\nMature Applications\n\nThe final point I'd like to cover is the maturity of applications developed on\ntop of any given platform. This is where modern languages fall short as by\ndefinition they simply haven't gained enough penetration for mature\napplications to have been developed yet. This is where there is opportunity.\nThis is where the next generation of mature web applications will be built\nusing web application frameworks like Django\nand Rails that are designed from the ground up for\nrapid customization over long software life-cycles while maintaining the\noriginal design integrity of your application - something only a very\ndisciplined developer can maintain with something like PHP, which in almost\nall cases will eventually end up as spaghetti code.\n\nApplications like Wordpress and\nMagento may work fine off the shelf for an\nend user, but what type of path have you created for your customer by\nimplementing technology that gets closer and closer to its end of life the\nmore it's customized?\n\nIn conclusion - developers and CEOs, challenge your technical decision makers\nto overcome their complacency and invest in technology of the future. Push\nyour employer, your peers and most importantly yourself forward. Invest your\ntime in efficient, secure and unencumbered technology.", "published": 0, "publish_date": "2010-09-28T17:00:00", "slug": "on-modern-web-development", "title": "On Modern Web Development"}}, {"pk": 20, "model": "blog.blogpost", "fields": {"content": "I'm happy to announce the first release of\nCartridge - a\nDjango shopping cart application I started\nworking on back in 2009. The development path that Cartridge has taken has\nbeen a strange one. I stopped working on it throughout 2010 in order to get\nthe ball rolling with a project called Mezzanine\nthat I've blogged about previously. Many parts of Mezzanine actually\noriginated in Cartridge and once development of Mezzanine was well under way\nit made the most sense for continued development of Cartridge to occur as a\nplug-in for Mezzanine which has now come to fruition.\n\nBeyond creating a kick-ass shopping cart application for Django, the main\ndesign goal I originally had for Cartridge was to address some of the mistakes\nI felt existed in other offerings available both within and outside of the\nDjango community. These areas I've aimed to address are as follows:\n\nPerformance\n\nThe Django ORM is a\ndouble-edged sword that while saving you a lot of time can also do a lot of\ndamage when used without regard to the underlying SQL queries being performed.\nI've come across examples in templates where queries are being performed\ninside loops nested inside more loops resulting in abysmal performance. Fixing\nthese problems wasn't simply a case of refactoring template logic as these\nissues were core to the design of how prices and variations were modelled. The\nonly solution was to throw a ton of fine-grained\nmemcached usage at the problem, but this should be a\noptional approach to scalability - not a minimum requirement for keeping the\nsite from falling offline. Cartridge has been designed with performance in\nmind from the start with a range of denormalized data structures providing\nO(n) performance as the number of products and categories grow.\n\nIntuitive Interfaces\n\nAn end user should be able to use an admin system for the first time and\ndiscover an interface that is logical and intuitive. Having to go through a\nhandful of screens to set up a single product requires users to mentally train\nthemselves to remember a work-flow that isn't entirely natural. If the former\nis achievable then the latter is definitely unacceptable. The number of forms\nand fields in an interface can be described as a conversation between the\nsystem and its user, and this conversation should be as quick and painless as\npossible. Cartridge provides single interfaces for creating products,\ncategories, discounts and other types of shop data, with only applicable\nfields making up these forms.\n\nBloated Code\n\nHaving a system that implements every single feature that might ever be\nrequired in a shopping cart implementation certainly makes for an easy sell,\nhowever as this list grows these features become more obscure and less likely\nto be required in an average implementation. This can result in a convoluted\ncode base that is very difficult to apply customizations to - an inevitable\nrequirement given the unique nature of shopping cart implementations.\nCartridge addresses this issue by implementing only the features typically\nrequired by all shopping cart implementations, leaving custom features up to\nthe developer who will find that due to a tight feature list that the code\nbase and data models remain very clean and easy to work with.\n\nAside from these features that distinguish it from other shopping cart\napplications, Cartridge comes with a standard set of features that you'd\nexpect to find:\n\n\n  Hierarchical categories (via Mezzanine pages)\n  Easily configurable product options (colours, sizes, etc)\n  Hooks for shipping calculations and payment gateway\n  Sale pricing and discount codes\n  Stock management\n  Product popularity\n  Thumbnail generation\n  Built-in test suite\n  Separation of presentation (no embedded markup)\n  Smart categories (by price range, colour, etc)\n  Configurable number of checkout steps\n\n\nThe live demo of Mezzanine now includes\nCartridge so go ahead and try it out! If you're interested in hacking on\nCartridge then the source code is freely available under a BSD\nlicense at both\ngithub and\nbitbucket.", "published": 1, "publish_date": "2010-09-21T17:00:00", "slug": "plugging-in-cartridge", "title": "Plugging In Cartridge"}}, {"pk": 21, "model": "blog.blogpost", "fields": {"content": "You'd be forgiven for reading the title of this post and thinking it's about a\ncrazy approach to project briefing that somehow mimics open source development\n- as interesting as that sounds, it isn't the case and my motives are much\nmore simplistic and sinister. What I'd like to do here is put a brief together\nfor an open source project called Mezzanine. This\nbrief isn't specifically geared towards programmers so if you think this isn't\nfor you then please continue reading and let me prove you wrong.\n\nWhat is Mezzanine?\n\nAnyone who follows my updates will know it's\nan open source CMS framework I've been working on over the last couple of\nmonths. It now has a concrete feature\nset having come\nremarkably far in a very short amount of time. This might lead you to believe\nan entire team of people have been working on it but in fact it's mostly been\nmyself alone - it's thanks to the incredibly rapid development that using\nDjango brings you that so much has been done so\nquickly. For those readers who aren't familiar with it please go ahead and\ncheck out the overview in the\ndocumentation, play around with the live\ndemo and have a read of my previous blog\npost that\ntalks about why I started Mezzanine and what I hope to achieve.\n\nWhy would I want to help?\n\nPerhaps you're an end user of a poorly designed CMS and you've often wished\nyou could do something about it. Perhaps you're a developer that's had the\nunfortunate experience of trying to extend a seemingly user-friendly CMS\nthat's built using archaic technology, and wished you could be working with\nsomething that's much more cutting edge and elegantly designed. Perhaps you're\nsomeone who 'gets' open source at a deeper level but always felt as someone\nwho isn't a coder that you couldn't contribute. Perhaps you're in business\ndevelopment and you're tired of trying to sell 'enterprise' crap with\ncompletely absurd price tags. If you have anything to do with web development\nthen there's something in this for you.\n\nWhat do I get out of it?\n\nAs much as you put in of course. The experience of contributing to open source\nsoftware on paper can often be a competitive advantage over other candidates\nfor a job interview or even development contracts for your business. There's\nalso the chance of notoriety - imagine being responsible for the user\ninterface or branding of the next Wordpress. Imagine\nyour staff are core contributors to one of the web's leading development\ntools. Again the success of the project will only match its contributions so\nit's ultimately up to you.\n\nWhat can I do to help?\n\nA common misconception about open source software is that it's something that\nonly coders can participate in. Unfortunately the result of this is that the\nmajority of open source software ends up being only contributed to by coders\nand is incredibly lacking in a variety of areas. I'm talking about visual\nbranding, copy-writing, UI development - all these areas that fall outside of\ncoding but are equally crucial in successfully shipping a professional piece\nof software. Mezzanine has now reached a point where it can only continue to\nmove forward at a consistent pace by bringing in these skills that I don't\nspecialise in. So without further ado, here are the specific roles I think\nneed filling and what the focus of each would be.\n\nGraphic Designer\n\nThe entire project is desperately in need of some visual love. At the simplest\nlevel it could really use some basic branding such as a logo and 'powered by'\nbuttons. Then there's the Mezzanine website,\ndocumentation and default\nsite that are all currently quite spartan\nlooking.\n\nInterface Developer\n\nSo far the template mark-up for the default site is as minimal as can be.\nWhile this is intentional to some extent in order to best serve those that\nwould come along and customise it for their projects, I think this idea could\nbe improved upon with a greater level of modularity. I'm also keen to\nintroduce a CSS framework like Blueprint into the\ndefault site. Once that's all in order then I'd like to address what theming\nwould look like. Is this simply a matter of packaging up copies of the\ntemplates as separate themes? A great milestone for Mezzanine would be to have\na handful of built-in themes created, as well as documenting the process for\ncreating your own.\n\nUX Designer\n\nI've introduced a handful of user interface elements into Mezzanine that could\ndefinitely use some ironing out from a usability and accessibility\nperspective. The main contender is the navigation tree in the admin that's\nused for managing the hierarchy of the entire site as well as being the entry\npoint for accessing most of the content management. There's the dashboard\ninterface for the admin area which is in a very early stage. There's the\noverall layout for both the project's own site\nand the default site. Lastly and of great\nimportance, there's the entire system for in-line\nediting which is featured\nin the default site - making this feature as user-friendly as possible is\ncritical.\n\nTechnical Writer\n\nMezzanine currently has a good start on\ndocumentation but at the moment it's mostly\nfocused on developers. I'd eventually like to have a lot more material aimed\nat both end users of Mezzanine as well as marketing material geared towards\nbusiness decision makers.\n\nProduct Evangelist\n\nThis is probably the easiest task of all. We simply need the word to be\nspread. Learn about Mezzanine and use whatever medium you like to let the\nworld know how great it is, be it Twitter, mailing lists\nor blog posts.\n\nThis list isn't entirely complete and some of the tasks certainly overlap. If\nyou think you fit the bill or know anyone else who would get a kick out of\nworking on Mezzanine then there's no time like the present to get started.\nThere aren't any obligations with this so contributions of any size are\nwelcome. If you'd like to get involved but don't know where to start just\npost a message to the mailing list and let's talk!", "published": 1, "publish_date": "2010-07-22T17:00:00", "slug": "an-open-source-brief", "title": "An Open Source Brief"}}, {"pk": 22, "model": "blog.blogpost", "fields": {"content": "Ask any developer that has put together a Django admin\ninterface and they'll\ntell you that it's an amazing piece of technology that allows you to whip up\nan admin system for your web application in a number of minutes rather than\ndays. Unfortunately this power can be a double-edged sword as without enough\ntime and thought up front, a developer can easily end up creating an interface\nthat's almost impossible for its intended audience to work with.\n\nI've seen this issue manifest itself in two ways. The first is the bare-bones\ncase where the options available for creating the admin interface are simply\nleft out, resulting in a spartan admin that does little to guide the user on\nhow to use it. The second could be described as the opposite end of the scale\nwhere there is actually far too much going on in the admin interface at the\ncost of simplicity and intuitiveness. This can often be the result of gluing\ntogether a range of resuable apps that each have their own approach to\nproviding admin interfaces with the end result looking like a Rube Goldberg\nmachine. These scenarios\nare bad for customers and bad for Django. When end-users think of Django, they\nthink of the admin interface - that's what Django is to them so it's critical\nto get this component right.\n\nI recently had Wordpress suggested to me as a\nsolution to this problem and it's easy to see why. The Wordpress install base\nalone speaks huge volumes while its admin interface is incredibly user-\nfriendly. It also benefits from not requiring technical expertise to get a\nsimple website with pages and a blog up and running. However I felt this idea\noverlooked the underlying issue of poorly configured Django admin interfaces,\nwhile taking a step backwards by investing in PHP - a\nrelatively inelegant technology with a very limited application scope.\n\nMy solution to the problem was to tackle the underlying issue more directly by\ncreating a Django application which I've called\nMezzanine. The approach I've taken\nis to have functionality on par with Wordpress that can be used as a starting\npoint when developing basic websites. This meant putting a lot of thought into\nthe admin options used, as well as including a custom version of the django-\ngrappelli admin skin to come up\nwith a modern looking and intuitive admin interface. The other key approach\nI've taken is to include as much functionality as possible directly in the\napplication itself for the sake of a consistent and lightweight code base that\ncan easily be hacked on. It's worth noting that this is in total contrast to\nother Django website applications such as\nMingus and\nPinax, and that this difference really comes down\nto a question of scope. Pinax for example is capable of a much wider range of\nfunctionality than what I'm aiming for with Mezzanine out of the box which is\nto cater for basic websites with the following features:\n\n\n  Hierarchical page navigation\n  Save as draft and preview on site\n  Drag-n-drop page ordering\n  WYSIWYG editing\n  SEO friendly URLs and meta data\n  Mobile device detection and templates\n  Blogging engine\n  Tagging\n  Custom templates per page or blog post\n  Gravatar integration\n  Google Analytics integration\n  Twitter feed integration\n  bit.ly integration\n  Sharing via Facebook or Twitter\n  Built-in threaded comments, or:\n  Disqus integration\n\n\nThe Mezzanine admin dashboard\n\nI've open sourced the initial version of Mezzanine with a BSD license on both\ngithub and\nbitbucket - it still has a long\nway to go so jump right in and fork away.", "published": 1, "publish_date": "2010-06-11T17:00:00", "slug": "mezzanine-just-another-django-cms", "title": "Mezzanine: Just Another Django CMS?"}}, {"pk": 23, "model": "blog.blogpost", "fields": {"content": "Like a lot of Django shops our software stack consists of two layers up front:\na public facing web/proxy server and an application server sitting behind it.\nFor a long time we've enjoyed success using nginx and\nApache to fill these roles respectively, but as an\napplication server the 800 pound gorilla that is Apache can really be\noverkill, which over time we've found can have quite a cost around lack of\ngranular control. So we recently decided to try out the up and coming\ngunicorn which is currently gaining in popularity\nthroughout the Django community and so far it's been very smooth.\n\nOne of the interesting features it provides is the ability to handle various\nkill signals which map to functions such as adding and removing worker\nprocesses as well as reloading the master process, all on the fly without\ndropping a single client connection. So after a brief honeymoon period I then\ncame up with the following list of questions that mightn't be apparent when\nserving a single application, but really come into play when serving dozens of\napplications this way on a single server:\n\n\n  How can we deal with the signals interface without knowledge of process IDs?\n  How can we gain visiblity around the ports being used?\n  How can we gain visiblity around the number of worker procesess being used?\n  How can we gain visiblity around the amount of memory being used per application?\n\n\nAll of these can be answered with a small amount of command-line-fu, however I\nwanted this process to be ridiculously easy for our entire team. For quite\nsome time I've wanted to put together a console application using the curses\nlibrary so a simple management\nconsole for gunicorn seemed like the perfect opportunity to do so and as such,\ngunicorn-console was born.\n\n\n\nAs pictured above, after firing up a few gunicorn instances with varying\nparameters gunicorn-console gives you the following interface in all its 8bit\nglory:\n\n\n\nIf you're hosting multiple applications served up via gunicorn then gunicorn-\nconsole should make managing them easier. I've released it with a BSD license\non both github and\nbitbucket using the\namazing hg-git extension, so go ahead and make it\nbetter!\n\nUpdate, May 30: I ended this post with a request for others to contribute and after only a day someone already has. Adam Vandenberg went ahead and forked the project with some patches to get it running on OSX, so a big thanks goes to him.", "published": 1, "publish_date": "2010-05-29T17:00:00", "slug": "announcing-gunicorn-console", "title": "Announcing gunicorn-console"}}, {"pk": 24, "model": "blog.blogpost", "fields": {"content": "A project of mine contains a number of third-party apps that are development\nrelated and potentially not available on every machine the project will run\non. My general approach to dealing with these was to try and import the app in\nmy settings module and if successful, add it to the INSTALLED_APPS\nsetting. However as the number of these apps grew it became a wart within the\nsettings module so I put together this snippet for managing them.\n\nWe first create a sequence of dictionaries, each containing information about\nan installed app such as the module to try and import, an extra potential\ncondition for checking and then the sequences of names to add to\nINSTALLED_APPS, MIDDLEWARE_CLASSES and TEMPLATE_CONTEXT_PROCESSORS.\nLet's start with the settings for optionally including the apps django-\ncommand-extensions,\ndjango-debug-toolbar and\nsouth.\n\n# Define any settings specific to each of the optional apps.\nimport sys\nUSE_SOUTH = not (len(sys.argv) &gt; 1 and sys.argv[1] == \"test\")\nDEBUG_TOOLBAR_CONFIG = {\"INTERCEPT_REDIRECTS\": False}\n\n# Sequence for each optional app as a dict containing info about the app.\nOPTIONAL_APPS = (\n    {\"import\": \"django_extensions\", \"apps\": (\"django_extensions\",)},\n    {\"import\": \"debug_toolbar\", \"apps\": (\"debug_toolbar\",), \n        \"middleware\": (\"debug_toolbar.middleware.DebugToolbarMiddleware\",)},\n    {\"import\": \"south\", \"apps\": (\"south\",), \"condition\": USE_SOUTH},\n)\n\n\n\nNext we simply iterate through the sequence of optional apps and set them up.\n\n# Set up each optional app if available.\nfor app in OPTIONAL_APPS:\n    if app.get(\"condition\", True):\n        try:\n            __import__(app[\"import\"])\n        except ImportError:\n            pass\n        else:\n            INSTALLED_APPS += app.get(\"apps\", ())\n            MIDDLEWARE_CLASSES += app.get(\"middleware\", ())\n\n", "published": 1, "publish_date": "2010-05-10T17:00:00", "slug": "optional-django-apps", "title": "Optional Django Apps"}}, {"pk": 25, "model": "blog.blogpost", "fields": {"content": "Anyone who has programmed in Python for a considerable length of time will at\nleast have some passing familiarity with PEP\n8 - the document that goes into an\nincredible level of detail in dictating precisely how code should be written.\nWhile its primary goal is to ensure that Python code is written in a\nconsistant fashion throughout the community, therefore making it as easy as\npossible to read, it also provides one of many aspects that makes programming\nin Python an incredibly efficient process - it negates the need for a lot of\ndecision making around any of the choices one might come across that are\nalready covered in PEP 8.\n\nI've recently been spending a lot of time writing technical documentation.\nWhile it's been interesting doing something different for a change, the\nperfectionist in me is constantly frustated with finding myself using\ninconsistant language across different sections when faced with the exact same\ncontext, for example:\n\n\n  contains / has / includes / provides\n  discussed / referred to / described / mentioned\n  above / earlier / previously\n  below / later / next\n\n\nAre these types of ambiguities in technical writing something that\nprofessional editors typically deal with? What I'd love to see is something\nlike PEP 8 for technical documentation.", "published": 1, "publish_date": "2010-03-18T17:00:00", "slug": "where-is-pep-8-for-technical-documentation", "title": "Where is PEP 8 for Technical Documentation?"}}, {"pk": 26, "model": "blog.blogpost", "fields": {"content": "Earlier this week I had the pleasure of removing my final Windows install\nafter wiping my machine at work and installing Ubuntu on it. It was during the\nlate 90s that I first tried out Linux after getting my hands on a Redhat 6.1\nCD from the cover of a magazine I'd bought. I didn't keep it installed for\nvery long and after a few more tries over the years with Mandrake (now\nMandrivia) and Damn Small Linux, it wasn't until 2005 when I installed\nSlackware 10.2 as my primary operating system at home and really cut my teeth\non it in order to test how cross-platform my Python projects were. It was a\ngreat experience learning about all the various sub-systems, compiling\nsoftware and libraries from source, embracing the command line and modifying\nsome of the internal scripts to get things working the way I wanted.\n\nFast forward to 2010 and in my workplace the migration from a Microsoft\ndevelopment shop to a Linux/Python shop after several years is finally\ncomplete, paving the way for this latest install. I did experience a couple of\nhiccups that hadn't happened before. Firstly I have dual wide-screen monitors\nat work and I rotate one of them 90 degrees in order to maximize the amount of\nvisible code on my screen. The display properties in Ubuntu only gave me the\nability to flip the display 180 degrees which seemed quite odd so I then tried\nto manually rotate the display with the\nxrandr command which reported my overall\nvirtual screen size as being too small for the rotation. I resolved this with\nthe update below to my x.org configuration to use a virtual screen size large\nenough to handle the rotation while including the second monitor.\n\nSection \"Screen\"\n    Identifier \"Configured Screen Device\"\n    Device \"Configured Video Device\"\n    SubSection \"Display\"\n        Virtual 2880 1440\n    EndSubSection\nEndSection\n\n\n\nThe second issue was more a lack of foresight on my part than a problem with\nthe new install itself. After a vanilla install of any modern operating system\nyou'll undoubtedly be required to download a series of updates that have\noccurred since the version you've installed was initially released. The\ndifference with most Linux distributions is that almost all of your software\nis managed in this way from installing to updating, it all goes through the\nsame service known as a package manager - one of the many things with Linux\nthat once you're used to using you won't know how you ever worked without it.\nSo away I went with the initial round of updates which left the package\nmanager busy for several hours, during which time some issues arose with a\nproject that immediately required my attention. Unfortunately I needed to\ninstall a handful of libraries to get up and running and with the package\nmanager busy I was left in a real bind. Fortunately I was able to use one of\nour test servers remotely to resolve the issue but the lesson learnt here is\nthat for a new development system it's best to leave the initial system update\nuntil after your development environment is completely set up.", "published": 1, "publish_date": "2010-02-13T18:00:00", "slug": "hasta-la-vista-windows", "title": "Hasta la Vista, Windows"}}, {"pk": 27, "model": "blog.blogpost", "fields": {"content": "My current project has the common requirement of storing and rendering a\nhierarchical tree of categories. This project is geared towards potentially\njunior developers with the expectation of it being hacked at every time it's\nused - a set of scaffolding where simplicity isn't just a loose goal but a\nfundamental requirement.\n\nTwo popular approaches to the hierarchical tree are the Adjacency List (AL)\nand Modified Preorder Tree Traversal (MPTT)\nmodels. The\nadvantage of AL is that it only stores the exact data required for\nrepresenting the tree while MPTT stores extraneous data for assisting in\ntraversing the tree in an optimal fashion. The simplicity of the AL model\nmakes it much better suited to the requirements I mentioned, however the\nproblem with AL is the recursive nature in which you traverse it.\n\ndef show_branch(parent, depth=0):\n    # iterating the entire tree for each branch gives quadratic performance\n    for node in nodes:\n        if node.parent == parent:\n            print (\" \" * depth) + node\n            show_branch(node, depth + 1)\n\n\n\nWorst case here is O(n\u00b2) performance but thanks to Python's lightning fast\nhashtable implementation we can\ncreate a copy of the tree as a dictionary of branches giving us O(n) overall\nperformance when traversing the entire tree.\n\n# copy the tree into a dict of branches\nbranches = {}\nfor node in nodes:\n    parent = node.parent\n    if parent not in branches:\n        branches[parent] = []\n    branches[parent].append(node)\n\ndef show_branch(parent, depth=0):\n    # iterating only the nodes in the branch gives linear performance\n    for node in branches[parent]:\n        print (\" \" * depth) + node\n        if node in branches:\n            show_branch(node, depth + 1)\n\n\n\nWhen rendering the entire tree, using this technique will greatly increase\nperformance as the tree grows in size. Be aware though that if your\napplication only ever deals with a single branch in any given view, this\ntechnique won't perform as well as directly querying the database for the\nnodes in a single branch.", "published": 1, "publish_date": "2010-01-26T18:00:00", "slug": "linear-traversal-of-adjacency-list-trees", "title": "Linear Traversal of Adjacency List Trees"}}, {"pk": 28, "model": "blog.blogpost", "fields": {"content": "Technology shows style of things to come\n\nThe Sydney Morning Herald has an article that talks about some of the work I\ndid for Sportsgirl including their community portal running on Django and a\nFacebook app written in C# - nice to get recognized!", "published": 1, "publish_date": "2009-11-30T18:00:00", "slug": "technology-shows-style-of-things-to-come", "title": "Technology Shows Style of Things to Come"}}, {"pk": 29, "model": "blog.blogpost", "fields": {"content": "Many major Linux distributions such as Ubuntu ship with\ngedit as the default text editor. It has\nall the standard features you'd expect in an editor such as syntax\nhighlighting, a tabbed interface and the ability to integrate external tools.\nMost importantly it's highly extensible with the ability to create plug-\nins for it written in Python or C.\n\nOne great plug-in that's been written is gedit-ftp-\nbrowser, an FTP client that\nembeds itself into the editor giving you the ability to remotely edit files\nover FTP. I've just been accepted as a contributor to the project which I'm\nreally excited about. I've implemented a 'Save As'\nfeature allowing the\nuser to create and upload new files over FTP. Next up I'll be working on the\nability to manage multiple FTP servers via profiles.", "published": 1, "publish_date": "2009-11-03T18:00:00", "slug": "ftp-browser-for-gedit", "title": "FTP Browser for gedit"}}, {"pk": 30, "model": "blog.blogpost", "fields": {"content": "Lately I've noticed people posting various different takes on making the\ndefault Django settings a lot more dynamic. The development and deployment\nrequirements for the projects I work on tend to be far from straight-forward\nand over time I've come up with my own approach to Django settings, so here it\nis.\n\nThe simplest approach typically involves importing all the names from a custom\nsettings module at the end of the project's standard settings module,\nproviding the ability to override settings on a per machine basis.\n\ntry:\n    from local_settings import *\nexcept ImportError:\n    pass\n\n\n\nThis still requires modifying local_settings.py on a per machine basis.\nAnother approach that builds on this is to import a custom settings module\nfrom a host_settings package that has a unique name derived from the current\nmachine the site is running on - this gives the advantage of being able to\nspecify custom settings per machine and have each of these settings modules\nreside in the version control system, without the same file having to be\nmodified on a per machine basis.\n\nfrom socket import gethostname\ntry:\n    exec (\"from host_settings.%s import *\" % \n        gethostname().replace(\"-\", \"_\").replace(\".\", \"_\"))\nexcept ImportError:\n    pass\n\n\n\nThis simple version of the host_settings approach I've seen attempts to deal\nwith the differences between a valid hostname and a valid module name with the\ntwo calls to replace, but ignores the fact a hostname could begin with a\nnumber which would be an invalid module name. Other versions of this approach\nhandle this correctly and involve calling the __import__ built-in, iterating\nover and updating each name in the settings module individually, but once we\nlook at some further requirements below and how to deal with them we'll find\nthis isn't necessary.\n\nLet's take a step back for a moment and talk about some of the requirements I\nmentioned before. Where I work a project can end up deployed in a dozen\ndifferent locations - a handful of developer machines and various different\nservers managing the project life cycle. Due to a variety of non-technical\nreasons it's often required that various versions of a project run side by\nside in the same location, so with a project called project_x we end up with\nproject_x_feature_a and project_x_feature_b sitting in the same location -\nall of a sudden all of our references to project_x are broken. So we end up\ntaking the approach in our code that the actual name of a project's directory\nis a moving target and should never be referenced - we never import from\npackage_x and anything in our settings module that would typically reference\nthis we set dynamically.\n\nimport os\nproject_path = os.path.dirname(os.path.abspath(__file__))\nproject_dir = project_path.split(os.sep)[-1]\nMEDIA_URL = \"/site_media/\"\nMEDIA_ROOT = os.path.join(project_path, MEDIA_URL.strip(\"/\"))\nTEMPLATE_DIRS = (os.path.join(project_path, \"templates\"),)\nADMIN_MEDIA_PREFIX = \"/media/\"\nROOT_URLCONF = \"%s.urls\" % project_dir\n\n\n\nSo that removes any hard-coded reference to the project's directory name,\nhowever we sometimes have to go as far as having host specific settings that\nvary across these different project versions residing on the same machine,\nsuch as a different database for example. The ultimate goal here is to not\nhave any files in the project's version control repository that are manually\nedited for a specific instance of the project. So using the host_settings\napproach from earlier on, we continue on from the code above by using the\nproject_dir variable when referencing the machine specific host_settings\nmodule so that each of the host_settings modules are named not only after\nthe machine they exist for, but the project directory as well.\n\nfrom socket import gethostname\nhost_settings_module = \"%s_%s\" % (project_dir, \n    gethostname().replace(\"-\", \"_\").replace(\".\", \"_\").lower())\nhost_settings_path = os.path.join(project_path, \"host_settings\", \n    \"%s.py\" % host_settings_module)\nif not os.path.exists(host_settings_path):\n    try:\n        f = open(host_settings_path, \"w\")\n        f.close()\n    except IOError:\n        print \"couldnt create host_settings module: %s \" % host_settings_path\ntry:\n    exec \"from host_settings.%s import *\" % host_settings_module\nexcept ImportError:\n    pass\nTEMPLATE_DEBUG = DEBUG\n\n\n\nAs an added bonus, we try to create the host_settings module if it's missing\nand warn if we're unable to create it.", "published": 1, "publish_date": "2009-10-20T17:00:00", "slug": "dynamic-django-deployment", "title": "Dynamic Django Deployment"}}, {"pk": 31, "model": "blog.blogpost", "fields": {"content": "After living the last few years without a mobile phone it finally became a\nproblem for me so recently I decided to get a new one. I only needed something\nsimple for receiving calls, not making them so any kind of plan or contract\nwas out of the question since I could spend a few bucks on pre-paid and\ntheoretically not pay anything after that. I didn't consider any of the latest\nsmart-phones either like the HTC or iPhone (I wouldn't buy a crippled device\nanyway) since they're so ridiculously priced compared to lower end models that\naren't even a year old.\n\nI ended up getting a Nokia E63 that has a full qwerty keyboard and wireless LAN access\nwhich were the main selling points for me. It cost $299 when it was being sold\nfor over $500 at other places around town so it felt like a smart purchase.\nApart from those features I looked at, I really knew nothing about the phone\nand Symbian OS which powers it and after taking a\ncloser look at the software available for it I've been really surprised.\n\n Google Apps for\nSymbian OS  Putty for Symbian OS\n Python (PyS60) on\nSymbian OS\n\nGoogle has created a handful of Symbian\napps for things like\ngmail, reader, youtube and maps which all work great. The other day I found an\napp called fring which is very similar to\nPidgin in that it wraps up all the various IM clients\ninto one, even including Skype with voice working! So it has essentially\nturned my phone into a Skype handset which is amazing. I've also found that\nthere's a version of putty for Symbian so I\ncan actually SSH onto any of our Linux servers or desktops and access the\nshell from my phone!\n\nThe most incredible thing I've found for Symbian however has been a project\ncalled PyS60 - Nokia has actually\nported the Python run-time to the Symbian OS. I was amazed once I had this\ninstalled and was sitting there typing out Python code into an interactive\nconsole directly on my phone. The standard library is even included and it's\nvery interesting - certain pieces aren't fully ported but it comes with a\nhandful of modules specifically for PyS60 which handle things like locating\nwireless networks and working with the user interface. It even includes\nOpenGL bindings which is unbelievable - that's\nright, you can develop 3D games in Python for your phone.\n\nI'm well into developing my first app which is a small RPC server for the\nphone controlled by a wxWidgets client. The idea is\nto be able to traverse the phone's file system and create, edit and execute\nPython apps on the phone from a remote machine. The SimpleXMLRPCServer module\nisn't included with PyS60 and broke when I tried to copy it onto the phone and\nimport it manually. I've since been able to patch it and get it working which\nI've submitted to Nokia, hopefully they'll include it in their\nnext release.", "published": 1, "publish_date": "2009-09-26T17:00:00", "slug": "nokia-e63", "title": "Nokia E63"}}, {"pk": 32, "model": "blog.blogpost", "fields": {"content": "I think CHM files are great and my main Python reference is the CHM version of\nthe main documentation. I\u2019ve used GnoCHM\nwhich is the default CHM viewer for Gnome for quite a while and it\u2019s really\npoor - slow as hell on startup and segfaults half the time you click a link. I\nfinally gave up and looked for an alternative which I found in\nKchmViewer which appears to be the default CHM\nviewer for KDE. As usual the KDE counterpart of a given Gnome app is much\neasier on the eyes and in this case the problems I had are solved - lightning\nfast and stable.", "published": 1, "publish_date": "2009-09-21T17:00:00", "slug": "chm-files-on-linux", "title": "CHM Files on Linux"}}]